

<!DOCTYPE html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"
><head>
  <meta charset="utf-8" />
  
    <meta http-equiv="content-language" content="en" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  
  <title>Predict heart attack outcomes using decision tree classifier from scratch and scikit-learn &middot; Ilija Eftimov üë®‚ÄçüöÄ</title>
    <meta name="title" content="Predict heart attack outcomes using decision tree classifier from scratch and scikit-learn &middot; Ilija Eftimov üë®‚ÄçüöÄ" />
  
  <meta name="description" content="Documenting my experiences and learnings, with the goal of helping other software engineers on their journey" />
  
  
  
  <link rel="canonical" href="https://ieftimov.com/posts/classifier-decision-trees/" />
  
  
  
  
  
  
  
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.4801cb831d6359e43865e37733fc89df21983ad1c08157d4e17c36e09cb2501cdbd2e1dcde1db016cfa1554f175134d948555ab487816811cb9e93b19673ce89.css"
    integrity="sha512-SAHLgx1jWeQ4ZeN3M/yJ3yGYOtHAgVfU4Xw24JyyUBzb0uHc3h2wFs&#43;hVU8XUTTZSFVatIeBaBHLnpOxlnPOiQ=="
  />
  
  
  <script type="text/javascript" src="/js/appearance.min.badab316c9287a5a42a843e4eb45da65bb3d194a5a0f5fa4a3e516160e67df0b8c65f4f19a8e146436e29d583699e6cb41d6bbe99e05e1dbaa877763bad9f8e2.js" integrity="sha512-utqzFskoelpCqEPk60XaZbs9GUpaD1&#43;ko&#43;UWFg5n3wuMZfTxmo4UZDbinVg2mebLQda76Z4F4duqh3djutn44g=="></script>
  
    
    
    
  
  
    
    
  
  
  
    
    <script defer type="text/javascript" id="script-bundle" src="/js/main.bundle.min.15ae6e2c9b1ac24a9ccf40003fa689efb1a18db1ee9b73d780b01a6c31b150441415862513e93184f68fe385759e4698b8763cba6a0f79493c1fed99ad5868d4.js" integrity="sha512-Fa5uLJsawkqcz0AAP6aJ77GhjbHum3PXgLAabDGxUEQUFYYlE&#43;kxhPaP44V1nkaYuHY8umoPeUk8H&#43;2ZrVho1A==" data-copy="Copy" data-copied="Copied"></script>
  
  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
    <link rel="manifest" href="/site.webmanifest" />
  
  
  
  
  
  
  
  <meta property="og:title" content="Predict heart attack outcomes using decision tree classifier from scratch and scikit-learn" />
<meta property="og:description" content="Learn the k-Nearest Neighbors algorithm with me with a practical application on a dataset with diabetes patients" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ieftimov.com/posts/classifier-decision-trees/" /><meta property="og:image" content="https://ieftimov.com/posts/classifier-decision-trees/index_files/card.jpg" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-01T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-03-01T00:00:00+00:00" /><meta property="og:site_name" content="Ilija Eftimov üë®‚ÄçüöÄ" />

  <meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://ieftimov.com/posts/classifier-decision-trees/index_files/card.jpg"/>

<meta name="twitter:title" content="Predict heart attack outcomes using decision tree classifier from scratch and scikit-learn"/>
<meta name="twitter:description" content="Learn the k-Nearest Neighbors algorithm with me with a practical application on a dataset with diabetes patients"/>

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Posts",
    "name": "Predict heart attack outcomes using decision tree classifier from scratch and scikit-learn",
    "headline": "Predict heart attack outcomes using decision tree classifier from scratch and scikit-learn",
    "description": "Learn the k-Nearest Neighbors algorithm with me with a practical application on a dataset with diabetes patients",
    "abstract": "While studying for my IBM Data Science Profesional Certificate on Coursera, I came across decision trees - a robust classification and regression algorithm.",
    "inLanguage": "en",
    "url" : "https:\/\/ieftimov.com\/posts\/classifier-decision-trees\/",
    "author" : {
      "@type": "Person",
      "name": "Ilija Eftimov"
    },
    "copyrightYear": "2023",
    "dateCreated": "2023-03-29T00:00:00\u002b00:00",
    "datePublished": "2023-03-01T00:00:00\u002b00:00",
    
    "dateModified": "2023-03-01T00:00:00\u002b00:00",
    
    
    
    "mainEntityOfPage": "true",
    "wordCount": "5618"
  }]
  </script>


  
  <meta name="author" content="Ilija Eftimov" />
  
    
      <link href="https://twitter.com/ilijaio" rel="me" />
    
      <link href="https://www.linkedin.com/in/ieftimov/" rel="me" />
    
      <link href="mailto:blog@ieftimov.com" rel="me" />
    
      <link href="https://captainscodebook.com" rel="me" />
    
      <link href="https://dev.to/fteem" rel="me" />
    
      <link href="https://github.com/fteem" rel="me" />
    
      <link href="https://t.me/ieftimovcom" rel="me" />
    
  
  
  






  
  <script async defer data-domain="ieftimov.com" src="https://plausible.io/js/plausible.js"></script>

  
  
</head>
<body
    class="flex flex-col h-screen px-6 m-auto text-lg leading-7 bg-neutral text-neutral-900 sm:px-14 md:px-24 lg:px-32 dark:bg-neutral-800 dark:text-neutral max-w-7xl"
  >
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 dark:bg-neutral-600 focus:translate-y-0"
        href="#main-content"
        ><span class="font-bold ltr:pr-2 rtl:pl-2 text-primary-600 dark:text-primary-400"
          >&darr;</span
        >Skip to main content</a
      >
    </div>
    
    
      <header class="py-6 font-semibold sm:py-10 text-neutral-900 dark:text-neutral print:hidden">
  <nav class="flex justify-between">
    
    <div>
      
        <a
          class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
          rel="me"
          href="/"
          >Ilija Eftimov üë®‚ÄçüöÄ</a
        >
      

    </div>
    
    
      <ul class="flex flex-col list-none ltr:text-right rtl:text-left sm:flex-row">
        
          
            <li class="mb-1 sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
                href="/"
                title=""
                >Home</a
              >
            </li>
          
            <li class="mb-1 sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
                href="/posts/"
                title="Posts"
                >Essays</a
              >
            </li>
          
            <li class="mb-1 sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
                href="/work"
                title=""
                >Work</a
              >
            </li>
          
            <li class="mb-1 sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
                href="https://captainscodebook.com"
                title=""
                >Subscribe</a
              >
            </li>
          
        
        
          <li class="ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0">
            <button
              id="search-button"
              class="text-base hover:text-primary-600 dark:hover:text-primary-400"
              title="Search (/)"
            >
              

  <span class="relative inline-block align-text-bottom icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


            </button>
          </li>
        
      </ul>
    
  </nav>
</header>

    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  <article>
    <header class="max-w-prose">
      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        Predict heart attack outcomes using decision tree classifier from scratch and scikit-learn
      </h1>
      <div class="mt-8 mb-12 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
    
  

  

  

  
    
  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="2023-03-29 00:00:00 &#43;0000 UTC">29 March 2023</time><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">27 mins</span>
    

    
    
  </div>

  
  


      </div>
    </header>
    <section class="flex flex-col max-w-full mt-0 prose lg:flex-row dark:prose-invert">
      
        <div class="order-first px-0 lg:max-w-xs ltr:lg:pl-8 rtl:lg:pr-8 lg:order-last">
          <div class="ltr:pl-5 rtl:pr-5 toc lg:sticky lg:top-10 print:hidden">
            <details open class="mt-0 overflow-hidden rounded-lg rtl:pr-5 ltr:pl-5 ltr:-ml-5 rtl:-mr-5">
  <summary
    class="block py-1 text-lg font-semibold cursor-pointer rtl:pr-5 ltr:pl-5 ltr:-ml-5 rtl:-mr-5 text-neutral-800 dark:text-neutral-100 lg:hidden bg-neutral-100 dark:bg-neutral-700"
  >
    Table of Contents
  </summary>
  <div
    class="py-2 border-dotted ltr:border-l rtl:border-r rtl:pr-5 ltr:pl-5 ltr:-ml-5 rtl:-mr-5 border-neutral-300 dark:border-neutral-600"
  >
    <nav id="TableOfContents">
  <ul>
    <li><a href="#decision-trees-101">Decision trees: 101</a></li>
    <li><a href="#decision-trees-from-scratch">Decision Trees, from scratch</a></li>
    <li><a href="#feature-splitting">Feature splitting</a></li>
    <li><a href="#building-a-tree">Building a tree</a></li>
    <li><a href="#using-a-subset-of-features">Using a subset of features</a>
      <ul>
        <li><a href="#minimum-samples-per-split">Minimum samples per split</a></li>
      </ul>
    </li>
    <li><a href="#predictions">Predictions</a></li>
    <li><a href="#predicting-cardiac-arrest">Predicting cardiac arrest</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
  </div>
</details>

          </div>
        </div>
      
      <div class="min-w-0 min-h-0 max-w-prose">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<p>While studying for my <a href="https://www.coursera.org/professional-certificates/ibm-data-science">IBM Data Science Profesional Certificate</a> on Coursera, I came across decision trees - a robust classification and regression algorithm. Like k-Nearest Neighbors, decision trees are a shallow learning algorithm because they don&rsquo;t require multi-layer modeling. In fact, like k-NN, decision trees also depend on how the algorithm will fit the features/dataset internally, so it can then make predictions on out-of-set data.</p>
<p>This post aims to look deeply at decision trees from a beginner&rsquo;s angle: the theory, implementation from scratch, applying the algorithm to a problem/dataset, and evaluating its accuracy and performance. Ultimately, we will also compare my implementation of decision trees against the one shipped with scikit-learn.</p>
<p>Before we begin, it&rsquo;s worth noting that decision trees can be used for both, classification and regression problems. Here I will explore only classification and look into regression in another post.</p>
<p>Let&rsquo;s begin.</p>
<h2 id="decision-trees-101" class="relative group">Decision trees: 101 <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#decision-trees-101" aria-label="Anchor">#</a></span></h2>
<p>To understand decision trees better, we need to first figure out how they work on an abstract level. Let&rsquo;s look at this imaginary dataset:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">import</span> <span style="color:#8fbcbb">pandas</span> <span style="color:#81a1c1;font-weight:bold">as</span> <span style="color:#8fbcbb">pd</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>data <span style="color:#81a1c1">=</span> <span style="color:#eceff4">{</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a3be8c">&#39;age&#39;</span><span style="color:#eceff4">:</span> <span style="color:#eceff4">[</span><span style="color:#b48ead">22</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">45</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">33</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">28</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">55</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">65</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">38</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">24</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">35</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">42</span><span style="color:#eceff4">],</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a3be8c">&#39;income&#39;</span><span style="color:#eceff4">:</span> <span style="color:#eceff4">[</span><span style="color:#b48ead">25000</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">50000</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">75000</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">40000</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">90000</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">120000</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">60000</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">30000</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">80000</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">95000</span><span style="color:#eceff4">],</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a3be8c">&#39;bought&#39;</span><span style="color:#eceff4">:</span> <span style="color:#eceff4">[</span><span style="color:#a3be8c">&#39;N&#39;</span><span style="color:#eceff4">,</span> <span style="color:#a3be8c">&#39;Y&#39;</span><span style="color:#eceff4">,</span> <span style="color:#a3be8c">&#39;Y&#39;</span><span style="color:#eceff4">,</span> <span style="color:#a3be8c">&#39;N&#39;</span><span style="color:#eceff4">,</span> <span style="color:#a3be8c">&#39;Y&#39;</span><span style="color:#eceff4">,</span> <span style="color:#a3be8c">&#39;Y&#39;</span><span style="color:#eceff4">,</span> <span style="color:#a3be8c">&#39;N&#39;</span><span style="color:#eceff4">,</span> <span style="color:#a3be8c">&#39;N&#39;</span><span style="color:#eceff4">,</span> <span style="color:#a3be8c">&#39;Y&#39;</span><span style="color:#eceff4">,</span> <span style="color:#a3be8c">&#39;Y&#39;</span><span style="color:#eceff4">]</span>
</span></span><span style="display:flex;"><span><span style="color:#eceff4">}</span>
</span></span><span style="display:flex;"><span>dummy_df <span style="color:#81a1c1">=</span> pd<span style="color:#81a1c1">.</span>DataFrame<span style="color:#eceff4">(</span>data<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>dummy_df
</span></span></code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>income</th>
      <th>bought</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>22</td>
      <td>25000</td>
      <td>N</td>
    </tr>
    <tr>
      <th>1</th>
      <td>45</td>
      <td>50000</td>
      <td>Y</td>
    </tr>
    <tr>
      <th>2</th>
      <td>33</td>
      <td>75000</td>
      <td>Y</td>
    </tr>
    <tr>
      <th>3</th>
      <td>28</td>
      <td>40000</td>
      <td>N</td>
    </tr>
    <tr>
      <th>4</th>
      <td>55</td>
      <td>90000</td>
      <td>Y</td>
    </tr>
    <tr>
      <th>5</th>
      <td>65</td>
      <td>120000</td>
      <td>Y</td>
    </tr>
    <tr>
      <th>6</th>
      <td>38</td>
      <td>60000</td>
      <td>N</td>
    </tr>
    <tr>
      <th>7</th>
      <td>24</td>
      <td>30000</td>
      <td>N</td>
    </tr>
    <tr>
      <th>8</th>
      <td>35</td>
      <td>80000</td>
      <td>Y</td>
    </tr>
    <tr>
      <th>9</th>
      <td>42</td>
      <td>95000</td>
      <td>Y</td>
    </tr>
  </tbody>
</table>
</div>
<p>It shows customer purchases of a particular product. Each observation contains the customer&rsquo;s age and income and whether they bought the product.How can we use this data to build a tree that, when provided with a customer&rsquo;s age and income, will predict whether they will buy the product or not?</p>
<p>Imagine we try to split the dataset into multiple parts and start with all observations - they all belong to the root node. We then ask the dataset a binary question for one of its features. Next, we split the observations of the dataset based on the answer to the question &ndash; the &lsquo;yes&rsquo; rows go to the left node, and the &rsquo;no&rsquo; rows go to the right node. We then create child nodes and assign them the rows for each of the &lsquo;yes&rsquo; and &rsquo;no&rsquo; rows. Finally, we continue this process recursively until we get to leaf nodes - nodes that are (mostly) pure (of the same class).</p>
<p>The steps outlined above are the process we go through when building decision trees. We split off the dataset into different branches, trying to get to a place where each leaf node is (almost) pure, or in other words, has all the rows belonging to the same class/label.</p>
<p>We&rsquo;re overlooking lots of important details here, but that&rsquo;s it in a nutshell. Visually, the decision tree for our dummy dataset would look like this:</p>
<p><img src="index_files/decision-tree-graphic.png" alt=""  />
</p>
<p>If we jump ahead and just use scikit-learn&rsquo;s <code>DecisionTreeClassifier</code>, make it fit our dummy dataset, and build a decision tree out of it, it would look like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">from</span> <span style="color:#8fbcbb">sklearn.tree</span> <span style="color:#81a1c1;font-weight:bold">import</span> DecisionTreeClassifier
</span></span><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">from</span> <span style="color:#8fbcbb">sklearn.tree</span> <span style="color:#81a1c1;font-weight:bold">import</span> plot_tree
</span></span><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">import</span> <span style="color:#8fbcbb">matplotlib.pyplot</span> <span style="color:#81a1c1;font-weight:bold">as</span> <span style="color:#8fbcbb">plt</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#616e87;font-style:italic"># Convert categorical variables to numerical using one-hot encoding</span>
</span></span><span style="display:flex;"><span>plot_df <span style="color:#81a1c1">=</span> pd<span style="color:#81a1c1">.</span>get_dummies<span style="color:#eceff4">(</span>dummy_df<span style="color:#eceff4">,</span> columns<span style="color:#81a1c1">=</span><span style="color:#eceff4">[</span><span style="color:#a3be8c">&#39;bought&#39;</span><span style="color:#eceff4">])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#616e87;font-style:italic"># Create decision tree classifier</span>
</span></span><span style="display:flex;"><span>dtree <span style="color:#81a1c1">=</span> DecisionTreeClassifier<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#616e87;font-style:italic"># Fit the decision tree classifier on the data</span>
</span></span><span style="display:flex;"><span>X <span style="color:#81a1c1">=</span> plot_df<span style="color:#81a1c1">.</span>drop<span style="color:#eceff4">([</span><span style="color:#a3be8c">&#39;bought_N&#39;</span><span style="color:#eceff4">,</span> <span style="color:#a3be8c">&#39;bought_Y&#39;</span><span style="color:#eceff4">],</span> axis<span style="color:#81a1c1">=</span><span style="color:#b48ead">1</span><span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>y <span style="color:#81a1c1">=</span> plot_df<span style="color:#eceff4">[</span><span style="color:#a3be8c">&#39;bought_Y&#39;</span><span style="color:#eceff4">]</span>
</span></span><span style="display:flex;"><span>dtree<span style="color:#81a1c1">.</span>fit<span style="color:#eceff4">(</span>X<span style="color:#eceff4">,</span> y<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#616e87;font-style:italic"># Plot the decision tree</span>
</span></span><span style="display:flex;"><span>fig<span style="color:#eceff4">,</span> ax <span style="color:#81a1c1">=</span> plt<span style="color:#81a1c1">.</span>subplots<span style="color:#eceff4">(</span>figsize<span style="color:#81a1c1">=</span><span style="color:#eceff4">(</span><span style="color:#b48ead">12</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">8</span><span style="color:#eceff4">),</span> dpi<span style="color:#81a1c1">=</span><span style="color:#b48ead">60</span><span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>plot_tree<span style="color:#eceff4">(</span>dtree<span style="color:#eceff4">,</span> feature_names<span style="color:#81a1c1">=</span>X<span style="color:#81a1c1">.</span>columns<span style="color:#eceff4">,</span> impurity<span style="color:#81a1c1">=</span><span style="color:#81a1c1;font-weight:bold">False</span><span style="color:#eceff4">,</span> class_names<span style="color:#81a1c1">=</span><span style="color:#eceff4">[</span><span style="color:#a3be8c">&#39;No&#39;</span><span style="color:#eceff4">,</span> <span style="color:#a3be8c">&#39;Yes&#39;</span><span style="color:#eceff4">],</span> proportion<span style="color:#81a1c1">=</span><span style="color:#81a1c1;font-weight:bold">False</span><span style="color:#eceff4">,</span> filled<span style="color:#81a1c1">=</span><span style="color:#81a1c1;font-weight:bold">True</span><span style="color:#eceff4">,</span> ax<span style="color:#81a1c1">=</span>ax<span style="color:#eceff4">,</span> label<span style="color:#81a1c1">=</span><span style="color:#a3be8c">&#39;all&#39;</span><span style="color:#eceff4">,</span> rounded<span style="color:#81a1c1">=</span><span style="color:#81a1c1;font-weight:bold">True</span><span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#81a1c1">.</span>show<span style="color:#eceff4">()</span>
</span></span></code></pre></div><p><img src="index_files/figure-markdown_strict/cell-3-output-1.png" alt=""  />
</p>
<p>We can see that scikit-learn built a decision tree for us - it split off the dataset on the <code>income &lt;= 67500</code> decision at the root node. Then, on the right side, it immediately got a pure node with the &lsquo;Yes&rsquo; class, while on the left side, it needed another decision, this time on <code>age &lt;= 41.5</code>. For the rows that answered &lsquo;No&rsquo;, it split those samples off to the left while the &lsquo;Yes&rsquo; rows went to the right node.</p>
<p>We can see that each node has several samples assigned to it - those are the observations/rows of the dataset that went into that node. The label of each sample set to a node is the class that the node represents. And that&rsquo;s all about how decision tree work in theory. We&rsquo;ll dive into more details next, such as how we can choose what feature (e.g., <code>income</code>) and threshold (e.g., <code>67500</code>) to split our dataset on.</p>
<h2 id="decision-trees-from-scratch" class="relative group">Decision Trees, from scratch <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#decision-trees-from-scratch" aria-label="Anchor">#</a></span></h2>
<p>Let&rsquo;s go through a rudimentary implementation of decision trees. At the beginning of this post, I put k-NN and decision trees in the same bucket, but that&rsquo;s deceiving. k-NN is simple; decision trees are complex. So make yourself a warm beverage and cozy up, as this post section will turn up the heat.</p>
<p>If you have a background in computer science, then this might be familiar: decision trees are a tree data structure, just on steroids. The different decision tree algorithms implement them as binary trees for practical reasons. In other words, every tree node will have a left and a right pointer to another node in the tree. So, a node will be a simple structure that will contain pointers to the left and right nodes, the feature that the node represents, its threshold (we&rsquo;ll see this in a bit), and the value/class of the node (only if a leaf node).</p>
<p>To implement decision trees, we&rsquo;ll need to implement the <code>fit</code> and <code>predict</code> methods (or their equivalents). Once invoked, the <code>fit</code> method will build out the whole decision tree based on the train data provided (features and labels). The decision tree algorithm is recursive by nature, so <code>fit</code> will recursively build the nested nodes and invoke itself for every next node it needs to make. <code>predict</code> will also need to recursively traverse the tree and return the classification based on the detected node.</p>
<p>I&rsquo;ve seen many folks online tackling the decision tree algorithms in one go, which works fine. Still, I prefer to break it down to the core behaviors and components that allow decision trees to work as they do. Those are:</p>
<ul>
<li>Decision tree configuration, e.g., the allowed depth of the tree, splits per node, etc.</li>
<li>Building a binary tree using recursion, e.g., use correct stop criteria based on configuration and labels available</li>
<li>Figuring out how to split a feature and on what thresholds. There are different decision tree algorithms, but in the one we&rsquo;ll cover, we&rsquo;ll look at information gain and entropy.</li>
</ul>
<p>So, we will try to internalize these before we look at the algorithm as a whole. And we&rsquo;ll go in reverse order.</p>
<h2 id="feature-splitting" class="relative group">Feature splitting <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#feature-splitting" aria-label="Anchor">#</a></span></h2>
<p>Using our initial example of customer characteristics and whether they have purchased an item, we must figure out how to build the tree. To make a decision node, we must find the feature to split the dataset (e.g., <code>Income</code>) and its threshold (e.g., <code>67500</code>). To get both, we must get all the features and labels and measure the point at which we want to perform the split.</p>
<p>Let&rsquo;s say we have the data frame from before:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dummy_df <span style="color:#81a1c1">=</span> pd<span style="color:#81a1c1">.</span>get_dummies<span style="color:#eceff4">(</span>dummy_df<span style="color:#eceff4">,</span> columns<span style="color:#81a1c1">=</span><span style="color:#eceff4">[</span><span style="color:#a3be8c">&#39;bought&#39;</span><span style="color:#eceff4">])</span><span style="color:#81a1c1">.</span>drop<span style="color:#eceff4">(</span>columns<span style="color:#81a1c1">=</span><span style="color:#eceff4">[</span><span style="color:#a3be8c">&#39;bought_N&#39;</span><span style="color:#eceff4">])</span><span style="color:#81a1c1">.</span>rename<span style="color:#eceff4">(</span>columns<span style="color:#81a1c1">=</span><span style="color:#eceff4">{</span><span style="color:#a3be8c">&#39;bought_Y&#39;</span><span style="color:#eceff4">:</span> <span style="color:#a3be8c">&#39;bought&#39;</span><span style="color:#eceff4">})</span>
</span></span><span style="display:flex;"><span>dummy_df
</span></span></code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>income</th>
      <th>bought</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>22</td>
      <td>25000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>45</td>
      <td>50000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>33</td>
      <td>75000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>28</td>
      <td>40000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>55</td>
      <td>90000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>65</td>
      <td>120000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>38</td>
      <td>60000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>24</td>
      <td>30000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>35</td>
      <td>80000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>9</th>
      <td>42</td>
      <td>95000</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
<p>To determine whether to use <code>age</code> or <code>income</code> and which threshold, we need to loop through each feature (<code>age</code> and <code>income</code>) and figure out the information gain between the feature and the target label (<code>bought</code>). Once we find the best gain from all the features, we will split the dataset on that feature with the found threshold.</p>
<p>Let&rsquo;s write this algorithm out. First, let&rsquo;s calculate information gain. Let&rsquo;s say we want to split our data on the <code>age</code> feature, with the threshold of <code>40</code>. In other words, we will split the above dataset into two parts: a part that contains all rows where <code>age &lt;= 40</code> and another part that contains all rows where <code>age &gt; 40</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>left_df <span style="color:#81a1c1">=</span> dummy_df<span style="color:#eceff4">[</span>dummy_df<span style="color:#eceff4">[</span><span style="color:#a3be8c">&#39;age&#39;</span><span style="color:#eceff4">]</span> <span style="color:#81a1c1">&lt;=</span> <span style="color:#b48ead">40</span><span style="color:#eceff4">]</span>
</span></span><span style="display:flex;"><span>left_df
</span></span></code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>income</th>
      <th>bought</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>22</td>
      <td>25000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>33</td>
      <td>75000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>28</td>
      <td>40000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>38</td>
      <td>60000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>24</td>
      <td>30000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>35</td>
      <td>80000</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>right_df <span style="color:#81a1c1">=</span> dummy_df<span style="color:#eceff4">[</span>dummy_df<span style="color:#eceff4">[</span><span style="color:#a3be8c">&#39;age&#39;</span><span style="color:#eceff4">]</span> <span style="color:#81a1c1">&gt;</span> <span style="color:#b48ead">40</span><span style="color:#eceff4">]</span>
</span></span><span style="display:flex;"><span>right_df
</span></span></code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>income</th>
      <th>bought</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>45</td>
      <td>50000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>55</td>
      <td>90000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>65</td>
      <td>120000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>9</th>
      <td>42</td>
      <td>95000</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
<p>If we look at the target value, we&rsquo;ll see that in the <code>right_df</code>, all rows contain <code>bought = 1</code>. But, in the <code>left_df</code>, we see a combination: four zeroes and two ones.</p>
<p>So was splitting the data on the <code>age</code> feature and threshold of <code>40</code> the right move? There&rsquo;s only one way to know: repeat the same process for all features and labels. But, even if we do that, we need a singular metric to compare across the different splits. Entropy and information gain are such metrics to aid us in the comparison.</p>
<p>Entropy in a collection describes the average level of information disorder or uncertainty. If you&rsquo;d like to oversimplify it, it&rsquo;s a metric explaining how messy the data is. On the other hand, information gain is the opposite of entropy - when entropy decreases, information gain increases. Information gain computes the difference in entropy before the split and average entropy after the dataset split based on given attribute values.</p>
<p>In other words, to know if the <code>age &lt;= 40</code> is the right split, we need to measure the disorder in the target variable (<code>bought</code>) before and after the split. If the entropy decreases, then the information gain will increase.</p>
<p>Let&rsquo;s see this in action:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">def</span> <span style="color:#88c0d0">calculate_entropy</span><span style="color:#eceff4">(</span>collection<span style="color:#eceff4">):</span>
</span></span><span style="display:flex;"><span>    ps <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>bincount<span style="color:#eceff4">(</span>collection<span style="color:#eceff4">)</span> <span style="color:#81a1c1">/</span> <span style="color:#81a1c1">len</span><span style="color:#eceff4">(</span>collection<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#81a1c1;font-weight:bold">return</span> <span style="color:#81a1c1">-</span><span style="color:#b48ead">1</span> <span style="color:#81a1c1">*</span> np<span style="color:#81a1c1">.</span>sum<span style="color:#eceff4">([</span>p <span style="color:#81a1c1">*</span> np<span style="color:#81a1c1">.</span>log<span style="color:#eceff4">(</span>p<span style="color:#eceff4">)</span> <span style="color:#81a1c1;font-weight:bold">for</span> p <span style="color:#81a1c1;font-weight:bold">in</span> ps <span style="color:#81a1c1;font-weight:bold">if</span> p <span style="color:#81a1c1">&gt;</span> <span style="color:#b48ead">0</span><span style="color:#eceff4">])</span>
</span></span></code></pre></div><p>The <code>calculate_entropy</code> method takes a <code>collection</code> (e.g.¬†a list) as an argument and calculates the collection&rsquo;s entropy. It does that by using the classic entropy forula:</p>
<p><img src="https://latex.codecogs.com/svg.latex?S%20%3D%20-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20p_i%20%5Cln%7Bp_i%7D" alt="S = -\sum_{i=1}^{n} p_i \ln{p_i}"  class="S = -\sum_{i=1}^{n} p_i \ln{p_i}" />
</p>
<p>If you prefer something other than mathematical notation, the Python code above should be of help. The entropy formula is the sum of the probabilities of each item in the collection multiplied by the natural logarithm of the probability of each item.</p>
<p>In the Python method <code>calculate_entropy</code>, Numpy&rsquo;s <code>bincount</code> method counts the occurrences of each item in the collection. Once we divide each count by the collection length, we get the probability for each item in the collection. From there on, we continue applying the formula by summing each probability&rsquo;s multiplication with the probability&rsquo;s natural logarithm. Finally, before returning the sum, we multiply it by -1, which is the last step of the formula.</p>
<p>Now that we have a reliable way to calculate entropy, we need to figure out how to calculate information gain. Remember, information gain is the reverse of entropy - as the entropy decreases, the information gain increases. So, to calculate information gain, we have to figure out the entropy of the data at a node before splitting it, then calculate the entropy of the children nodes after splitting their parent, and then subtract the children&rsquo;s entropy from the parent entropy - that will be the information gain. In other words, by subtracting the parent and the children&rsquo;s entropy, we calculate which direction the entropy moved after the parent node got split.</p>
<p>Can we find where to split the data into the children nodes? The answer is: yes, by brute forcing. We will take the whole dataset, and for each row, we will split off the dataset into two, calculate the information gain, go to the next row and split off the dataset into two again, and then calculate the information gain again, and so on until we find the best information gain. The index at which we find the best information gain is the index at which we will split the dataset.</p>
<p>Let&rsquo;s see this in code:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X <span style="color:#81a1c1">=</span> dummy_df<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> <span style="color:#eceff4">[</span><span style="color:#a3be8c">&#39;age&#39;</span><span style="color:#eceff4">,</span> <span style="color:#a3be8c">&#39;income&#39;</span><span style="color:#eceff4">]]</span>
</span></span><span style="display:flex;"><span>y <span style="color:#81a1c1">=</span> dummy_df<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> <span style="color:#a3be8c">&#39;bought&#39;</span><span style="color:#eceff4">]</span>
</span></span></code></pre></div><p>First, we are splitting our dummy dataset into the features (<code>X</code>) and the target labels (<code>y</code>). Next, we will only use the <code>age</code> column to figure out what is the best place to split it, or in other words, at what index is the information gain highest.</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#616e87;font-style:italic"># Find the best split for &#39;age&#39; column</span>
</span></span><span style="display:flex;"><span>X_age <span style="color:#81a1c1">=</span> X<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> <span style="color:#a3be8c">&#39;age&#39;</span><span style="color:#eceff4">]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#616e87;font-style:italic"># Get unique values, as checking for the same threshold N times is wasteful</span>
</span></span><span style="display:flex;"><span>thresholds <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>unique<span style="color:#eceff4">(</span>X_age<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">for</span> threshold <span style="color:#81a1c1;font-weight:bold">in</span> thresholds<span style="color:#eceff4">:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#616e87;font-style:italic"># Calculate entropy before split on &#39;age&#39; and &#39;40&#39;</span>
</span></span><span style="display:flex;"><span>    parent_entropy <span style="color:#81a1c1">=</span> calculate_entropy<span style="color:#eceff4">(</span>y<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#616e87;font-style:italic"># Split!</span>
</span></span><span style="display:flex;"><span>    left_X <span style="color:#81a1c1">=</span> X_age<span style="color:#eceff4">[</span>X_age <span style="color:#81a1c1">&lt;=</span> threshold<span style="color:#eceff4">]</span>
</span></span><span style="display:flex;"><span>    left_idx <span style="color:#81a1c1">=</span> left_X<span style="color:#81a1c1">.</span>index<span style="color:#81a1c1">.</span>to_list<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    right_X <span style="color:#81a1c1">=</span> X_age<span style="color:#eceff4">[</span>X_age <span style="color:#81a1c1">&gt;</span> threshold<span style="color:#eceff4">]</span>
</span></span><span style="display:flex;"><span>    right_idx <span style="color:#81a1c1">=</span> right_X<span style="color:#81a1c1">.</span>index<span style="color:#81a1c1">.</span>to_list<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    left_y<span style="color:#eceff4">,</span> right_y <span style="color:#81a1c1">=</span> y<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[</span>left_idx<span style="color:#eceff4">],</span> y<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[</span>right_idx<span style="color:#eceff4">]</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    left_entropy<span style="color:#eceff4">,</span> right_entropy <span style="color:#81a1c1">=</span> calculate_entropy<span style="color:#eceff4">(</span>left_y<span style="color:#eceff4">),</span> calculate_entropy<span style="color:#eceff4">(</span>right_y<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>    children_entropy <span style="color:#81a1c1">=</span> <span style="color:#eceff4">(</span><span style="color:#81a1c1">len</span><span style="color:#eceff4">(</span>left_y<span style="color:#eceff4">)</span><span style="color:#81a1c1">/</span><span style="color:#81a1c1">len</span><span style="color:#eceff4">(</span>y<span style="color:#eceff4">))</span> <span style="color:#81a1c1">*</span> left_entropy <span style="color:#81a1c1">+</span> <span style="color:#eceff4">(</span><span style="color:#81a1c1">len</span><span style="color:#eceff4">(</span>right_y<span style="color:#eceff4">)</span><span style="color:#81a1c1">/</span><span style="color:#81a1c1">len</span><span style="color:#eceff4">(</span>y<span style="color:#eceff4">))</span> <span style="color:#81a1c1">*</span> right_entropy
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    information_gain <span style="color:#81a1c1">=</span> parent_entropy <span style="color:#81a1c1">-</span> children_entropy
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#81a1c1">print</span><span style="color:#eceff4">(</span><span style="color:#a3be8c">f</span><span style="color:#a3be8c">&#34;Threshold: </span><span style="color:#a3be8c">{</span>threshold<span style="color:#a3be8c">}</span><span style="color:#a3be8c"> - Parent entropy: </span><span style="color:#a3be8c">{</span>parent_entropy<span style="color:#a3be8c">}</span><span style="color:#a3be8c"> - Children entropy: </span><span style="color:#a3be8c">{</span>children_entropy<span style="color:#a3be8c">}</span><span style="color:#a3be8c"> - Information gain: </span><span style="color:#a3be8c">{</span>information_gain<span style="color:#a3be8c">}</span><span style="color:#a3be8c">&#34;</span><span style="color:#eceff4">)</span>
</span></span></code></pre></div><pre><code>Threshold: 22 - Parent entropy: 0.6730116670092565 - Children entropy: 0.5728627514653315 - Information gain: 0.10014891554392502
Threshold: 24 - Parent entropy: 0.6730116670092565 - Children entropy: 0.44986811569504664 - Information gain: 0.22314355131420988
Threshold: 28 - Parent entropy: 0.6730116670092565 - Children entropy: 0.28708142280188625 - Information gain: 0.38593024420737027
Threshold: 33 - Parent entropy: 0.6730116670092565 - Children entropy: 0.4952707831673061 - Information gain: 0.17774088384195041
Threshold: 35 - Parent entropy: 0.6730116670092565 - Children entropy: 0.5867070452737222 - Information gain: 0.08630462173553433
Threshold: 38 - Parent entropy: 0.6730116670092565 - Children entropy: 0.38190850097688767 - Information gain: 0.29110316603236885
Threshold: 42 - Parent entropy: 0.6730116670092565 - Children entropy: 0.47803567329033014 - Information gain: 0.19497599371892638
Threshold: 45 - Parent entropy: 0.6730116670092565 - Children entropy: 0.5545177444479562 - Information gain: 0.11849392256130031
Threshold: 55 - Parent entropy: 0.6730116670092565 - Children entropy: 0.618265418937591 - Information gain: 0.054746248071665504
Threshold: 65 - Parent entropy: 0.6730116670092565 - Children entropy: 0.6730116670092565 - Information gain: 0.0
</code></pre>
<p>Evindently, if we ask the <code>age</code> column it will tell us that the best place to split the dataset at is on the row where the <code>age</code> has the value of <code>28</code>. In other words, feature to split at is <code>age</code> and the threshold is <code>28</code>.</p>
<p>We can repeate the same exercise for the <code>income</code> column:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#616e87;font-style:italic"># Find the best split for &#39;income&#39; column</span>
</span></span><span style="display:flex;"><span>X_income <span style="color:#81a1c1">=</span> X<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> <span style="color:#a3be8c">&#39;income&#39;</span><span style="color:#eceff4">]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#616e87;font-style:italic"># Get unique values, as checking for the same threshold N times is wasteful</span>
</span></span><span style="display:flex;"><span>thresholds <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>unique<span style="color:#eceff4">(</span>X_income<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">for</span> threshold <span style="color:#81a1c1;font-weight:bold">in</span> thresholds<span style="color:#eceff4">:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#616e87;font-style:italic"># Calculate entropy before split on &#39;income&#39;</span>
</span></span><span style="display:flex;"><span>    parent_entropy <span style="color:#81a1c1">=</span> calculate_entropy<span style="color:#eceff4">(</span>y<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#616e87;font-style:italic"># Split!</span>
</span></span><span style="display:flex;"><span>    left_X <span style="color:#81a1c1">=</span> X_income<span style="color:#eceff4">[</span>X_income <span style="color:#81a1c1">&lt;=</span> threshold<span style="color:#eceff4">]</span>
</span></span><span style="display:flex;"><span>    left_idx <span style="color:#81a1c1">=</span> left_X<span style="color:#81a1c1">.</span>index<span style="color:#81a1c1">.</span>to_list<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    right_X <span style="color:#81a1c1">=</span> X_income<span style="color:#eceff4">[</span>X_income <span style="color:#81a1c1">&gt;</span> threshold<span style="color:#eceff4">]</span>
</span></span><span style="display:flex;"><span>    right_idx <span style="color:#81a1c1">=</span> right_X<span style="color:#81a1c1">.</span>index<span style="color:#81a1c1">.</span>to_list<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    left_y<span style="color:#eceff4">,</span> right_y <span style="color:#81a1c1">=</span> y<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[</span>left_idx<span style="color:#eceff4">],</span> y<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[</span>right_idx<span style="color:#eceff4">]</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    left_entropy<span style="color:#eceff4">,</span> right_entropy <span style="color:#81a1c1">=</span> calculate_entropy<span style="color:#eceff4">(</span>left_y<span style="color:#eceff4">),</span> calculate_entropy<span style="color:#eceff4">(</span>right_y<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>    children_entropy <span style="color:#81a1c1">=</span> <span style="color:#eceff4">(</span><span style="color:#81a1c1">len</span><span style="color:#eceff4">(</span>left_y<span style="color:#eceff4">)</span><span style="color:#81a1c1">/</span><span style="color:#81a1c1">len</span><span style="color:#eceff4">(</span>y<span style="color:#eceff4">))</span> <span style="color:#81a1c1">*</span> left_entropy <span style="color:#81a1c1">+</span> <span style="color:#eceff4">(</span><span style="color:#81a1c1">len</span><span style="color:#eceff4">(</span>right_y<span style="color:#eceff4">)</span><span style="color:#81a1c1">/</span><span style="color:#81a1c1">len</span><span style="color:#eceff4">(</span>y<span style="color:#eceff4">))</span> <span style="color:#81a1c1">*</span> right_entropy
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    information_gain <span style="color:#81a1c1">=</span> parent_entropy <span style="color:#81a1c1">-</span> children_entropy
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#81a1c1">print</span><span style="color:#eceff4">(</span><span style="color:#a3be8c">f</span><span style="color:#a3be8c">&#34;Threshold: </span><span style="color:#a3be8c">{</span>threshold<span style="color:#a3be8c">}</span><span style="color:#a3be8c"> - Parent entropy: </span><span style="color:#a3be8c">{</span>parent_entropy<span style="color:#a3be8c">}</span><span style="color:#a3be8c"> - Children entropy: </span><span style="color:#a3be8c">{</span>children_entropy<span style="color:#a3be8c">}</span><span style="color:#a3be8c"> - Information gain: </span><span style="color:#a3be8c">{</span>information_gain<span style="color:#a3be8c">}</span><span style="color:#a3be8c">&#34;</span><span style="color:#eceff4">)</span>
</span></span></code></pre></div><pre><code>Threshold: 25000 - Parent entropy: 0.6730116670092565 - Children entropy: 0.5728627514653315 - Information gain: 0.10014891554392502
Threshold: 30000 - Parent entropy: 0.6730116670092565 - Children entropy: 0.44986811569504664 - Information gain: 0.22314355131420988
Threshold: 40000 - Parent entropy: 0.6730116670092565 - Children entropy: 0.28708142280188625 - Information gain: 0.38593024420737027
Threshold: 50000 - Parent entropy: 0.6730116670092565 - Children entropy: 0.4952707831673061 - Information gain: 0.17774088384195041
Threshold: 60000 - Parent entropy: 0.6730116670092565 - Children entropy: 0.25020121176909393 - Information gain: 0.4228104552401626
Threshold: 75000 - Parent entropy: 0.6730116670092565 - Children entropy: 0.38190850097688767 - Information gain: 0.29110316603236885
Threshold: 80000 - Parent entropy: 0.6730116670092565 - Children entropy: 0.47803567329033014 - Information gain: 0.19497599371892638
Threshold: 90000 - Parent entropy: 0.6730116670092565 - Children entropy: 0.5545177444479562 - Information gain: 0.11849392256130031
Threshold: 95000 - Parent entropy: 0.6730116670092565 - Children entropy: 0.618265418937591 - Information gain: 0.054746248071665504
Threshold: 120000 - Parent entropy: 0.6730116670092565 - Children entropy: 0.6730116670092565 - Information gain: 0.0
</code></pre>
<p>The same exercise again: asking the <code>income</code> column where to split will tell us that the row where the <code>income</code> has the value of <code>60000</code> is the best place to split the data. If we compare the best information gains of the <code>income</code> and <code>age</code> columns, we&rsquo;ll find out that the information gain on the <code>income</code> split is higher than the <code>age</code> split. Therefore, the first split we should perform on our data is on the <code>income &lt; 60000</code> threshold.</p>
<p>So now that we have applied the same algorithm to calculate the different entropies and information gain, let&rsquo;s generalize the code so it can find the best feature and threshold across multiple features:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">def</span> <span style="color:#88c0d0">find_best_split</span><span style="color:#eceff4">(</span>X<span style="color:#eceff4">,</span> y<span style="color:#eceff4">,</span> feat_idxs<span style="color:#eceff4">):</span>
</span></span><span style="display:flex;"><span>    best_gain <span style="color:#81a1c1">=</span> <span style="color:#81a1c1">-</span><span style="color:#b48ead">1</span>
</span></span><span style="display:flex;"><span>    best_feature_idx<span style="color:#eceff4">,</span> best_threshold <span style="color:#81a1c1">=</span> <span style="color:#81a1c1">-</span><span style="color:#b48ead">1</span><span style="color:#eceff4">,</span> <span style="color:#81a1c1">-</span><span style="color:#b48ead">1</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#81a1c1;font-weight:bold">for</span> feat_idx <span style="color:#81a1c1;font-weight:bold">in</span> feat_idxs<span style="color:#eceff4">:</span>
</span></span><span style="display:flex;"><span>        X_column <span style="color:#81a1c1">=</span> X<span style="color:#eceff4">[:,</span> feat_idx<span style="color:#eceff4">]</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#616e87;font-style:italic"># Get unique values, as checking for the same threshold N times is wasteful</span>
</span></span><span style="display:flex;"><span>        thresholds <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>unique<span style="color:#eceff4">(</span>X_column<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#616e87;font-style:italic"># Calculate entropy before split on &#39;income&#39;. Doing it here instead of on every loop.</span>
</span></span><span style="display:flex;"><span>        parent_entropy <span style="color:#81a1c1">=</span> calculate_entropy<span style="color:#eceff4">(</span>y<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#81a1c1;font-weight:bold">for</span> threshold <span style="color:#81a1c1;font-weight:bold">in</span> thresholds<span style="color:#eceff4">:</span>
</span></span><span style="display:flex;"><span>            <span style="color:#616e87;font-style:italic"># Split the column on the threshold</span>
</span></span><span style="display:flex;"><span>            left_X <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>where<span style="color:#eceff4">(</span>X_column <span style="color:#81a1c1">&lt;=</span> threshold<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>            right_X <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>where<span style="color:#eceff4">(</span>X_column <span style="color:#81a1c1">&gt;</span> threshold<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>            <span style="color:#616e87;font-style:italic"># Split the y, to two children splits</span>
</span></span><span style="display:flex;"><span>            left_y<span style="color:#eceff4">,</span> right_y <span style="color:#81a1c1">=</span> y<span style="color:#eceff4">[</span>left_X<span style="color:#eceff4">],</span> y<span style="color:#eceff4">[</span>right_X<span style="color:#eceff4">]</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>            <span style="color:#616e87;font-style:italic"># Calculate the entropy of the left and right children</span>
</span></span><span style="display:flex;"><span>            left_entropy<span style="color:#eceff4">,</span> right_entropy <span style="color:#81a1c1">=</span> calculate_entropy<span style="color:#eceff4">(</span>left_y<span style="color:#eceff4">),</span> calculate_entropy<span style="color:#eceff4">(</span>right_y<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#616e87;font-style:italic"># Calculate the average weighted children entropy</span>
</span></span><span style="display:flex;"><span>            children_entropy <span style="color:#81a1c1">=</span> <span style="color:#eceff4">(</span><span style="color:#81a1c1">len</span><span style="color:#eceff4">(</span>left_y<span style="color:#eceff4">)</span><span style="color:#81a1c1">/</span><span style="color:#81a1c1">len</span><span style="color:#eceff4">(</span>y<span style="color:#eceff4">))</span> <span style="color:#81a1c1">*</span> left_entropy <span style="color:#81a1c1">+</span> <span style="color:#eceff4">(</span><span style="color:#81a1c1">len</span><span style="color:#eceff4">(</span>right_y<span style="color:#eceff4">)</span><span style="color:#81a1c1">/</span><span style="color:#81a1c1">len</span><span style="color:#eceff4">(</span>y<span style="color:#eceff4">))</span> <span style="color:#81a1c1">*</span> right_entropy
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>            <span style="color:#616e87;font-style:italic"># Calculate the information gain</span>
</span></span><span style="display:flex;"><span>            information_gain <span style="color:#81a1c1">=</span> parent_entropy <span style="color:#81a1c1">-</span> children_entropy
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>            <span style="color:#81a1c1;font-weight:bold">if</span> information_gain <span style="color:#81a1c1">&gt;</span> best_gain<span style="color:#eceff4">:</span>
</span></span><span style="display:flex;"><span>                best_gain <span style="color:#81a1c1">=</span> information_gain
</span></span><span style="display:flex;"><span>                best_feature_idx <span style="color:#81a1c1">=</span> feat_idx
</span></span><span style="display:flex;"><span>                best_threshold <span style="color:#81a1c1">=</span> threshold
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#81a1c1;font-weight:bold">return</span> best_feature_idx<span style="color:#eceff4">,</span> best_threshold
</span></span></code></pre></div><p>The <code>find_best_split</code> method has the same functionality as the previous examples, with the key difference in that it has the ability to loop through many features (e.g.¬†both <code>age</code> and <code>income</code>) and find the best split across the different features.</p>
<p>Now that we have the <code>find_best_split</code> method defined, let&rsquo;s test it again on the <code>dummy_df</code> dataframe:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X <span style="color:#81a1c1">=</span> dummy_df<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> <span style="color:#eceff4">[</span><span style="color:#a3be8c">&#39;age&#39;</span><span style="color:#eceff4">,</span> <span style="color:#a3be8c">&#39;income&#39;</span><span style="color:#eceff4">]]</span><span style="color:#81a1c1">.</span>to_numpy<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>y <span style="color:#81a1c1">=</span> dummy_df<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> <span style="color:#a3be8c">&#39;bought&#39;</span><span style="color:#eceff4">]</span><span style="color:#81a1c1">.</span>to_numpy<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>best_feature<span style="color:#eceff4">,</span> best_threshold <span style="color:#81a1c1">=</span> find_best_split<span style="color:#eceff4">(</span>X<span style="color:#eceff4">,</span> y<span style="color:#eceff4">,</span> <span style="color:#eceff4">[</span><span style="color:#b48ead">0</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">1</span><span style="color:#eceff4">])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#81a1c1">print</span><span style="color:#eceff4">(</span><span style="color:#a3be8c">f</span><span style="color:#a3be8c">&#34;Best feature: </span><span style="color:#a3be8c">{</span>dummy_df<span style="color:#81a1c1">.</span>columns<span style="color:#eceff4">[</span>best_feature<span style="color:#eceff4">]</span><span style="color:#a3be8c">}</span><span style="color:#a3be8c">&#34;</span><span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span><span style="color:#81a1c1">print</span><span style="color:#eceff4">(</span><span style="color:#a3be8c">f</span><span style="color:#a3be8c">&#34;Best threshold: </span><span style="color:#a3be8c">{</span>best_threshold<span style="color:#a3be8c">}</span><span style="color:#a3be8c">&#34;</span><span style="color:#eceff4">)</span>
</span></span></code></pre></div><pre><code>Best feature: income
Best threshold: 60000
</code></pre>
<p>Awesome, we got the same result &ndash; for our dummy data frame, the best first split is on the <code>income</code> feature at the <code>60000</code> threshold. In other words, the first step to build the decision tree is to split the dataset on the <code>income</code> feature and set its <code>threshold</code> to <code>60000</code>. We will then have to repeat the same process recursively for both sides of the split, effectively building the tree.</p>
<p>Let&rsquo;s see how we can use the <code>find_best_split</code> method while building a binary tree recursively.</p>
<h2 id="building-a-tree" class="relative group">Building a tree <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#building-a-tree" aria-label="Anchor">#</a></span></h2>
<p>To build the tree, after getting the best feature and threshold for splitting, we need to use the feature and threshold to actually split the data and recursively continue to split the splits until it reaches the exit criteria. Which is what we&rsquo;ll look at next.</p>
<p>Recursion requires exit or stop criteria, which, when reached, will iterrupt the recursive process. For decision trees, the exit criteria will be to either set how deep the tree will go (i.e., how many levels it will have), how many features it can use to build the tree, or how many samples each node must have. So first, we can start with only a single exit criterion and build from there.</p>
<p>Looking at our dummy data, let&rsquo;s build see the next steps after the invocation of the <code>find_best_split</code> method:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X <span style="color:#81a1c1">=</span> dummy_df<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> <span style="color:#eceff4">[</span><span style="color:#a3be8c">&#39;age&#39;</span><span style="color:#eceff4">,</span> <span style="color:#a3be8c">&#39;income&#39;</span><span style="color:#eceff4">]]</span><span style="color:#81a1c1">.</span>to_numpy<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>y <span style="color:#81a1c1">=</span> dummy_df<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> <span style="color:#a3be8c">&#39;bought&#39;</span><span style="color:#eceff4">]</span><span style="color:#81a1c1">.</span>to_numpy<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>best_feature<span style="color:#eceff4">,</span> best_threshold <span style="color:#81a1c1">=</span> find_best_split<span style="color:#eceff4">(</span>X<span style="color:#eceff4">,</span> y<span style="color:#eceff4">,</span> <span style="color:#eceff4">[</span><span style="color:#b48ead">0</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">1</span><span style="color:#eceff4">])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>left_idxs <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>argwhere<span style="color:#eceff4">(</span>X<span style="color:#eceff4">[:,</span> best_feature<span style="color:#eceff4">]</span> <span style="color:#81a1c1">&lt;=</span> best_threshold<span style="color:#eceff4">)</span><span style="color:#81a1c1">.</span>flatten<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>right_idxs <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>argwhere<span style="color:#eceff4">(</span>X<span style="color:#eceff4">[:,</span> best_feature<span style="color:#eceff4">]</span> <span style="color:#81a1c1">&gt;</span> best_threshold<span style="color:#eceff4">)</span><span style="color:#81a1c1">.</span>flatten<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#81a1c1">print</span><span style="color:#eceff4">(</span><span style="color:#a3be8c">f</span><span style="color:#a3be8c">&#34;left idxs: </span><span style="color:#a3be8c">{</span>left_idxs<span style="color:#a3be8c">}</span><span style="color:#a3be8c">, right idxs: </span><span style="color:#a3be8c">{</span>right_idxs<span style="color:#a3be8c">}</span><span style="color:#a3be8c">&#34;</span><span style="color:#eceff4">)</span>
</span></span></code></pre></div><pre><code>left idxs: [0 1 3 6 7], right idxs: [2 4 5 8 9]
</code></pre>
<p>Now that we know the indexes of the rows that belong to the root&rsquo;s right and left child nodes let&rsquo;s prepare the child nodes. First, we will create a method <code>build_tree</code> that will take the <code>X</code> and <code>y</code> as arguments and will find the best feature/threshold pair; it will find the children node indices and then build the children nodes by recursively calling <code>build_tree</code>, with the subset of the <code>X</code> and `y&rsquo; using the split indices:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">def</span> <span style="color:#88c0d0">build_tree</span><span style="color:#eceff4">(</span>X<span style="color:#eceff4">,</span> y<span style="color:#eceff4">):</span>
</span></span><span style="display:flex;"><span>    best_feature<span style="color:#eceff4">,</span> best_threshold <span style="color:#81a1c1">=</span> find_best_split<span style="color:#eceff4">(</span>X<span style="color:#eceff4">,</span> y<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    left_idxs <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>argwhere<span style="color:#eceff4">(</span>X<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> best_feature<span style="color:#eceff4">]</span><span style="color:#81a1c1">.</span>to_numpy<span style="color:#eceff4">()</span> <span style="color:#81a1c1">&lt;=</span> best_threshold<span style="color:#eceff4">)</span><span style="color:#81a1c1">.</span>flatten<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>    right_idxs <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>argwhere<span style="color:#eceff4">(</span>X<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> best_feature<span style="color:#eceff4">]</span><span style="color:#81a1c1">.</span>to_numpy<span style="color:#eceff4">()</span> <span style="color:#81a1c1">&gt;</span> best_threshold<span style="color:#eceff4">)</span><span style="color:#81a1c1">.</span>flatten<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    left <span style="color:#81a1c1">=</span> build_tree<span style="color:#eceff4">(</span>X<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[</span>left_idxs<span style="color:#eceff4">],</span> y<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[</span>left_idxs<span style="color:#eceff4">])</span>
</span></span><span style="display:flex;"><span>    right <span style="color:#81a1c1">=</span> build_tree<span style="color:#eceff4">(</span>X<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[</span>right_idxs<span style="color:#eceff4">],</span> y<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[</span>right_idxs<span style="color:#eceff4">])</span>
</span></span></code></pre></div><p>That&rsquo;s all. Now we have the <code>left</code> and <code>right</code> pointers to the children. And the subsequent calls to <code>build_tree</code> will continiously build the tree until it depletes all rows of the <code>X</code> and <code>y</code> dataframes.</p>
<p>Our approach is naive. If we&rsquo;d run the code it would throw an out of bounds error because we never actually handle the exit criteria. Another problem is that even though we known how to identify the splits, we do not actually build a tree. The tree must be a data structure that later we can use for prediction making.</p>
<p>Let&rsquo;s define the data structure first, adapt our <code>build_tree</code> method, and then move on to the exit criteria.</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">class</span> <span style="color:#8fbcbb">TreeNode</span><span style="color:#eceff4">:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#81a1c1;font-weight:bold">def</span> __init__<span style="color:#eceff4">(</span>self<span style="color:#eceff4">,</span> feature<span style="color:#81a1c1">=</span><span style="color:#81a1c1;font-weight:bold">None</span><span style="color:#eceff4">,</span> threshold<span style="color:#81a1c1">=</span><span style="color:#81a1c1;font-weight:bold">None</span><span style="color:#eceff4">,</span> left<span style="color:#81a1c1">=</span><span style="color:#81a1c1;font-weight:bold">None</span><span style="color:#eceff4">,</span> right<span style="color:#81a1c1">=</span><span style="color:#81a1c1;font-weight:bold">None</span><span style="color:#eceff4">,</span> value<span style="color:#81a1c1">=</span><span style="color:#81a1c1;font-weight:bold">None</span><span style="color:#eceff4">):</span>
</span></span><span style="display:flex;"><span>        <span style="color:#616e87;font-style:italic"># Decision node</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#81a1c1">.</span>feature <span style="color:#81a1c1">=</span> feature
</span></span><span style="display:flex;"><span>        self<span style="color:#81a1c1">.</span>threshold <span style="color:#81a1c1">=</span> threshold
</span></span><span style="display:flex;"><span>        self<span style="color:#81a1c1">.</span>left <span style="color:#81a1c1">=</span> left
</span></span><span style="display:flex;"><span>        self<span style="color:#81a1c1">.</span>right <span style="color:#81a1c1">=</span> right
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#616e87;font-style:italic"># Leaf node</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#81a1c1">.</span>value <span style="color:#81a1c1">=</span> value
</span></span></code></pre></div><p>The <code>TreeNode</code> class will encapsulate the data required to construct the tree. Among the <code>feature</code> and the <code>threshold</code> used for splitting the dataset, it will also have the links to the <code>right</code> and the <code>left</code> children nodes. If the node is a leaf node, then it will also contin a value which is the class that the feature tree will predict.</p>
<p>Let&rsquo;s throw our <code>TreeNode</code> in the mix:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">def</span> <span style="color:#88c0d0">build_tree</span><span style="color:#eceff4">(</span>X<span style="color:#eceff4">,</span> y<span style="color:#eceff4">):</span>
</span></span><span style="display:flex;"><span>    best_feature<span style="color:#eceff4">,</span> best_threshold <span style="color:#81a1c1">=</span> find_best_split<span style="color:#eceff4">(</span>X<span style="color:#eceff4">,</span> y<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    left_idxs <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>argwhere<span style="color:#eceff4">(</span>X<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> best_feature<span style="color:#eceff4">]</span><span style="color:#81a1c1">.</span>to_numpy<span style="color:#eceff4">()</span> <span style="color:#81a1c1">&lt;=</span> best_threshold<span style="color:#eceff4">)</span><span style="color:#81a1c1">.</span>flatten<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>    right_idxs <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>argwhere<span style="color:#eceff4">(</span>X<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> best_feature<span style="color:#eceff4">]</span><span style="color:#81a1c1">.</span>to_numpy<span style="color:#eceff4">()</span> <span style="color:#81a1c1">&gt;</span> best_threshold<span style="color:#eceff4">)</span><span style="color:#81a1c1">.</span>flatten<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    left <span style="color:#81a1c1">=</span> build_tree<span style="color:#eceff4">(</span>X<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[</span>left_idxs<span style="color:#eceff4">],</span> y<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[</span>left_idxs<span style="color:#eceff4">])</span>
</span></span><span style="display:flex;"><span>    right <span style="color:#81a1c1">=</span> build_tree<span style="color:#eceff4">(</span>X<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[</span>right_idxs<span style="color:#eceff4">],</span> y<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[</span>right_idxs<span style="color:#eceff4">])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#81a1c1;font-weight:bold">return</span> TreeNode<span style="color:#eceff4">(</span>best_feature<span style="color:#eceff4">,</span> best_threshold<span style="color:#eceff4">,</span> left<span style="color:#eceff4">,</span> right<span style="color:#eceff4">)</span>
</span></span></code></pre></div><p>Awesome! But if would run the above method, we&rsquo;ll end up with an array indexing error. That&rsquo;s because we never check whether we&rsquo;ve depleted the indexes before we go for another recursive call.</p>
<p>To stop this class of problem, we need to implement the exit criteria. First, let&rsquo;s start by having the option to set the maximum depth of the tree:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">def</span> <span style="color:#88c0d0">build_tree</span><span style="color:#eceff4">(</span>X<span style="color:#eceff4">,</span> y<span style="color:#eceff4">,</span> current_depth<span style="color:#eceff4">,</span> max_depth<span style="color:#eceff4">):</span>
</span></span><span style="display:flex;"><span>    <span style="color:#81a1c1;font-weight:bold">if</span> current_depth <span style="color:#81a1c1">&gt;=</span> max_depth<span style="color:#eceff4">:</span>
</span></span><span style="display:flex;"><span>        counter <span style="color:#81a1c1">=</span> Counter<span style="color:#eceff4">(</span>y<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>        most_common_label <span style="color:#81a1c1">=</span> counter<span style="color:#81a1c1">.</span>most_common<span style="color:#eceff4">(</span><span style="color:#b48ead">1</span><span style="color:#eceff4">)[</span><span style="color:#b48ead">0</span><span style="color:#eceff4">][</span><span style="color:#b48ead">0</span><span style="color:#eceff4">]</span>
</span></span><span style="display:flex;"><span>        <span style="color:#81a1c1;font-weight:bold">return</span> TreeNode<span style="color:#eceff4">(</span>value<span style="color:#81a1c1">=</span>most_common_label<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    best_feature<span style="color:#eceff4">,</span> best_threshold <span style="color:#81a1c1">=</span> find_best_split<span style="color:#eceff4">(</span>X<span style="color:#eceff4">,</span> y<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    left_idxs <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>argwhere<span style="color:#eceff4">(</span>X<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> best_feature<span style="color:#eceff4">]</span><span style="color:#81a1c1">.</span>to_numpy<span style="color:#eceff4">()</span> <span style="color:#81a1c1">&lt;=</span> best_threshold<span style="color:#eceff4">)</span><span style="color:#81a1c1">.</span>flatten<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>    right_idxs <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>argwhere<span style="color:#eceff4">(</span>X<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> best_feature<span style="color:#eceff4">]</span><span style="color:#81a1c1">.</span>to_numpy<span style="color:#eceff4">()</span> <span style="color:#81a1c1">&gt;</span> best_threshold<span style="color:#eceff4">)</span><span style="color:#81a1c1">.</span>flatten<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    left <span style="color:#81a1c1">=</span> build_tree<span style="color:#eceff4">(</span>X<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[</span>left_idxs<span style="color:#eceff4">],</span> y<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[</span>left_idxs<span style="color:#eceff4">],</span> current_depth<span style="color:#81a1c1">+</span><span style="color:#b48ead">1</span><span style="color:#eceff4">,</span> max_depth<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>    right <span style="color:#81a1c1">=</span> build_tree<span style="color:#eceff4">(</span>X<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[</span>right_idxs<span style="color:#eceff4">],</span> y<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[</span>right_idxs<span style="color:#eceff4">],</span> current_depth<span style="color:#81a1c1">+</span><span style="color:#b48ead">1</span><span style="color:#eceff4">,</span> max_depth<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#81a1c1;font-weight:bold">return</span> TreeNode<span style="color:#eceff4">(</span>best_feature<span style="color:#eceff4">,</span> best_threshold<span style="color:#eceff4">,</span> left<span style="color:#eceff4">,</span> right<span style="color:#eceff4">)</span>
</span></span></code></pre></div><p>We added the <code>current_depth</code> and <code>max_depth</code> arguments to the <code>build_tree</code> method. The current depth is helpful for the recursion, so we can control at what depth each time we invoke the method. The maximum depth is valuable as the exit criteria, allowing us to control how deep the tree can branch out. In addition, the control of the depth is a powerful tool to prevent overfitting.</p>
<p>The rest of the <code>if</code> clause finds the most common label amongst the leftover labels (<code>y</code>). The <code>Counter</code> structure is useful for finding the most common item in a collection, which we then return as the <code>value</code> of a <code>TreeNode</code> - a leaf node!</p>
<p>We can also throw in other exit criteria, such as limiting the number of features we can use when building the decision tree. For example, say we have a dataset with many features - should we use them all? Some features are more valuable than the rest, and we should use only those while building the tree.</p>
<h2 id="using-a-subset-of-features" class="relative group">Using a subset of features <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#using-a-subset-of-features" aria-label="Anchor">#</a></span></h2>
<p>We need to make more dramatic changes to the <code>build_tree</code> and <code>find_best_split</code> methods to control the features we will use when building the decision tree. First, in the <code>find_best_split</code> method, we need to select a subset of features instead of using all the columns of the dependent variables (<code>X</code>). To do that, we will need to take a random feature subset.</p>
<p>Instead of just <code>income</code> and <code>age</code>, imagine our dataset had more features such as <code>income</code>, <code>age</code>, <code>education</code>, <code>married</code>, and <code>net_worth</code>; we could say that we want to use only a subset of them when building the decision tree. To do that, we would need to get a randomized subset using Numpy&rsquo;s <code>random.choice</code> method:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>features <span style="color:#81a1c1">=</span> <span style="color:#eceff4">[</span><span style="color:#a3be8c">&#39;income&#39;</span><span style="color:#eceff4">,</span> <span style="color:#a3be8c">&#39;age&#39;</span><span style="color:#eceff4">,</span> <span style="color:#a3be8c">&#39;education&#39;</span><span style="color:#eceff4">,</span> <span style="color:#a3be8c">&#39;married&#39;</span><span style="color:#eceff4">,</span> <span style="color:#a3be8c">&#39;net_worth&#39;</span><span style="color:#eceff4">]</span>
</span></span><span style="display:flex;"><span>n_useful_features <span style="color:#81a1c1">=</span> <span style="color:#b48ead">3</span> <span style="color:#616e87;font-style:italic"># Set as input to `build_tree`</span>
</span></span><span style="display:flex;"><span>repeat_features <span style="color:#81a1c1">=</span> <span style="color:#81a1c1;font-weight:bold">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>np<span style="color:#81a1c1">.</span>random<span style="color:#81a1c1">.</span>choice<span style="color:#eceff4">(</span>features<span style="color:#eceff4">,</span> n_useful_features<span style="color:#eceff4">,</span> repeat_features<span style="color:#eceff4">)</span>
</span></span></code></pre></div><pre><code>array(['age', 'education', 'net_worth'], dtype='&lt;U9')
</code></pre>
<p>By having this randomized array of feature names, we can use it on the <code>X</code> dataframe and just select a subset of the features. This will allow us to experiment with different tree configurations and find the best performing tree.</p>
<p>Let&rsquo;s see that in action in our <code>build_tree</code> and <code>find_best_split</code> methods. We will need to add the feature randomization in the <code>build_tree</code> method:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">def</span> <span style="color:#88c0d0">build_tree</span><span style="color:#eceff4">(</span>X<span style="color:#eceff4">,</span> y<span style="color:#eceff4">,</span> current_depth <span style="color:#81a1c1">=</span> <span style="color:#b48ead">0</span><span style="color:#eceff4">,</span> max_depth <span style="color:#81a1c1">=</span> <span style="color:#b48ead">5</span><span style="color:#eceff4">,</span> n_useful_features <span style="color:#81a1c1">=</span> <span style="color:#81a1c1;font-weight:bold">None</span><span style="color:#eceff4">):</span>
</span></span><span style="display:flex;"><span>    n_samples<span style="color:#eceff4">,</span> n_features <span style="color:#81a1c1">=</span> X<span style="color:#81a1c1">.</span>shape
</span></span><span style="display:flex;"><span>    n_labels <span style="color:#81a1c1">=</span> <span style="color:#81a1c1">len</span><span style="color:#eceff4">(</span>np<span style="color:#81a1c1">.</span>unique<span style="color:#eceff4">(</span>y<span style="color:#eceff4">))</span>
</span></span><span style="display:flex;"><span>    <span style="color:#81a1c1;font-weight:bold">if</span> n_useful_features <span style="color:#81a1c1">==</span> <span style="color:#81a1c1;font-weight:bold">None</span><span style="color:#eceff4">:</span> n_useful_features <span style="color:#81a1c1">=</span> n_features 
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#81a1c1;font-weight:bold">if</span> current_depth <span style="color:#81a1c1">&gt;=</span> max_depth <span style="color:#81a1c1;font-weight:bold">or</span> n_labels <span style="color:#81a1c1">==</span> <span style="color:#b48ead">1</span><span style="color:#eceff4">:</span>
</span></span><span style="display:flex;"><span>        counter <span style="color:#81a1c1">=</span> Counter<span style="color:#eceff4">(</span>y<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>        most_common_label <span style="color:#81a1c1">=</span> counter<span style="color:#81a1c1">.</span>most_common<span style="color:#eceff4">(</span><span style="color:#b48ead">1</span><span style="color:#eceff4">)[</span><span style="color:#b48ead">0</span><span style="color:#eceff4">][</span><span style="color:#b48ead">0</span><span style="color:#eceff4">]</span>
</span></span><span style="display:flex;"><span>        <span style="color:#81a1c1;font-weight:bold">return</span> TreeNode<span style="color:#eceff4">(</span>value<span style="color:#81a1c1">=</span>most_common_label<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#616e87;font-style:italic"># Select N random features</span>
</span></span><span style="display:flex;"><span>    feature_idxs <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>random<span style="color:#81a1c1">.</span>choice<span style="color:#eceff4">(</span>n_features<span style="color:#eceff4">,</span> n_useful_features<span style="color:#eceff4">,</span> replace<span style="color:#81a1c1">=</span><span style="color:#81a1c1;font-weight:bold">False</span><span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#616e87;font-style:italic"># Pass the feature subset of the X dataframe as argument to find_best_split</span>
</span></span><span style="display:flex;"><span>    best_feature<span style="color:#eceff4">,</span> best_threshold <span style="color:#81a1c1">=</span> find_best_split<span style="color:#eceff4">(</span>X<span style="color:#eceff4">,</span> y<span style="color:#eceff4">,</span> feature_idxs<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    left_idxs <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>argwhere<span style="color:#eceff4">(</span>X<span style="color:#eceff4">[:,</span> best_feature<span style="color:#eceff4">]</span> <span style="color:#81a1c1">&lt;=</span> best_threshold<span style="color:#eceff4">)</span><span style="color:#81a1c1">.</span>flatten<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>    right_idxs <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>argwhere<span style="color:#eceff4">(</span>X<span style="color:#eceff4">[:,</span> best_feature<span style="color:#eceff4">]</span> <span style="color:#81a1c1">&gt;</span> best_threshold<span style="color:#eceff4">)</span><span style="color:#81a1c1">.</span>flatten<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    left <span style="color:#81a1c1">=</span> build_tree<span style="color:#eceff4">(</span>X<span style="color:#eceff4">[</span>left_idxs<span style="color:#eceff4">],</span> y<span style="color:#eceff4">[</span>left_idxs<span style="color:#eceff4">],</span> current_depth<span style="color:#81a1c1">+</span><span style="color:#b48ead">1</span><span style="color:#eceff4">,</span> max_depth<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>    right <span style="color:#81a1c1">=</span> build_tree<span style="color:#eceff4">(</span>X<span style="color:#eceff4">[</span>right_idxs<span style="color:#eceff4">],</span> y<span style="color:#eceff4">[</span>right_idxs<span style="color:#eceff4">],</span> current_depth<span style="color:#81a1c1">+</span><span style="color:#b48ead">1</span><span style="color:#eceff4">,</span> max_depth<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#81a1c1;font-weight:bold">return</span> TreeNode<span style="color:#eceff4">(</span>best_feature<span style="color:#eceff4">,</span> best_threshold<span style="color:#eceff4">,</span> left<span style="color:#eceff4">,</span> right<span style="color:#eceff4">)</span>
</span></span></code></pre></div><h3 id="minimum-samples-per-split" class="relative group">Minimum samples per split <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#minimum-samples-per-split" aria-label="Anchor">#</a></span></h3>
<p>The last optimization that we&rsquo;ll make to our decision tree algorithm is to provide the control of the minimum samples that a node should have before it can be split. Such a limitation will make our trees less prone to overfitting - by setting such a threshold our tree building algorithm will not overfit the leaf nodes and branches.</p>
<p>Let&rsquo;s add this last configuration to the <code>build_tree</code> method:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">def</span> <span style="color:#88c0d0">build_tree</span><span style="color:#eceff4">(</span>X<span style="color:#eceff4">,</span> y<span style="color:#eceff4">,</span> current_depth <span style="color:#81a1c1">=</span> <span style="color:#b48ead">0</span><span style="color:#eceff4">,</span> max_depth <span style="color:#81a1c1">=</span> <span style="color:#b48ead">5</span><span style="color:#eceff4">,</span> n_useful_features <span style="color:#81a1c1">=</span> <span style="color:#81a1c1;font-weight:bold">None</span><span style="color:#eceff4">,</span> min_samples_split<span style="color:#81a1c1">=</span><span style="color:#b48ead">2</span><span style="color:#eceff4">):</span>
</span></span><span style="display:flex;"><span>    n_samples<span style="color:#eceff4">,</span> n_features <span style="color:#81a1c1">=</span> X<span style="color:#81a1c1">.</span>shape
</span></span><span style="display:flex;"><span>    n_labels <span style="color:#81a1c1">=</span> <span style="color:#81a1c1">len</span><span style="color:#eceff4">(</span>np<span style="color:#81a1c1">.</span>unique<span style="color:#eceff4">(</span>y<span style="color:#eceff4">))</span>
</span></span><span style="display:flex;"><span>    n_useful_features <span style="color:#81a1c1">=</span> n_features <span style="color:#81a1c1;font-weight:bold">if</span> <span style="color:#81a1c1;font-weight:bold">not</span> n_useful_features <span style="color:#81a1c1;font-weight:bold">else</span> <span style="color:#81a1c1">min</span><span style="color:#eceff4">(</span>n_features<span style="color:#eceff4">,</span> n_useful_features<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#81a1c1;font-weight:bold">if</span> <span style="color:#eceff4">(</span>current_depth <span style="color:#81a1c1">&gt;=</span> max_depth <span style="color:#81a1c1;font-weight:bold">or</span> n_labels <span style="color:#81a1c1">==</span> <span style="color:#b48ead">1</span> <span style="color:#81a1c1;font-weight:bold">or</span> n_samples<span style="color:#81a1c1">&lt;</span>min_samples_split<span style="color:#eceff4">):</span>
</span></span><span style="display:flex;"><span>        counter <span style="color:#81a1c1">=</span> Counter<span style="color:#eceff4">(</span>y<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>        most_common_label <span style="color:#81a1c1">=</span> counter<span style="color:#81a1c1">.</span>most_common<span style="color:#eceff4">(</span><span style="color:#b48ead">1</span><span style="color:#eceff4">)[</span><span style="color:#b48ead">0</span><span style="color:#eceff4">][</span><span style="color:#b48ead">0</span><span style="color:#eceff4">]</span>
</span></span><span style="display:flex;"><span>        <span style="color:#81a1c1;font-weight:bold">return</span> TreeNode<span style="color:#eceff4">(</span>value<span style="color:#81a1c1">=</span>most_common_label<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#616e87;font-style:italic"># Select N random features</span>
</span></span><span style="display:flex;"><span>    feature_idxs <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>random<span style="color:#81a1c1">.</span>choice<span style="color:#eceff4">(</span>n_features<span style="color:#eceff4">,</span> n_useful_features<span style="color:#eceff4">,</span> replace<span style="color:#81a1c1">=</span><span style="color:#81a1c1;font-weight:bold">False</span><span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#616e87;font-style:italic"># Pass the feature subset of the X dataframe as argument to find_best_split</span>
</span></span><span style="display:flex;"><span>    best_feature<span style="color:#eceff4">,</span> best_threshold <span style="color:#81a1c1">=</span> find_best_split<span style="color:#eceff4">(</span>X<span style="color:#eceff4">,</span> y<span style="color:#eceff4">,</span> feature_idxs<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    left_idxs <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>argwhere<span style="color:#eceff4">(</span>X<span style="color:#eceff4">[:,</span> best_feature<span style="color:#eceff4">]</span> <span style="color:#81a1c1">&lt;=</span> best_threshold<span style="color:#eceff4">)</span><span style="color:#81a1c1">.</span>flatten<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>    right_idxs <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>argwhere<span style="color:#eceff4">(</span>X<span style="color:#eceff4">[:,</span> best_feature<span style="color:#eceff4">]</span> <span style="color:#81a1c1">&gt;</span> best_threshold<span style="color:#eceff4">)</span><span style="color:#81a1c1">.</span>flatten<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    left <span style="color:#81a1c1">=</span> build_tree<span style="color:#eceff4">(</span>X<span style="color:#81a1c1">=</span>X<span style="color:#eceff4">[</span>left_idxs<span style="color:#eceff4">],</span> y<span style="color:#81a1c1">=</span>y<span style="color:#eceff4">[</span>left_idxs<span style="color:#eceff4">],</span> current_depth <span style="color:#81a1c1">=</span> current_depth<span style="color:#81a1c1">+</span><span style="color:#b48ead">1</span><span style="color:#eceff4">,</span> max_depth <span style="color:#81a1c1">=</span> max_depth<span style="color:#eceff4">,</span> n_useful_features <span style="color:#81a1c1">=</span> n_useful_features<span style="color:#eceff4">,</span> min_samples_split <span style="color:#81a1c1">=</span> min_samples_split<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>    right <span style="color:#81a1c1">=</span> build_tree<span style="color:#eceff4">(</span>X<span style="color:#81a1c1">=</span>X<span style="color:#eceff4">[</span>right_idxs<span style="color:#eceff4">],</span> y<span style="color:#81a1c1">=</span>y<span style="color:#eceff4">[</span>right_idxs<span style="color:#eceff4">],</span> current_depth <span style="color:#81a1c1">=</span> current_depth<span style="color:#81a1c1">+</span><span style="color:#b48ead">1</span><span style="color:#eceff4">,</span> max_depth <span style="color:#81a1c1">=</span> max_depth<span style="color:#eceff4">,</span> n_useful_features <span style="color:#81a1c1">=</span> n_useful_features<span style="color:#eceff4">,</span> min_samples_split <span style="color:#81a1c1">=</span> min_samples_split<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#81a1c1;font-weight:bold">return</span> TreeNode<span style="color:#eceff4">(</span>best_feature<span style="color:#eceff4">,</span> best_threshold<span style="color:#eceff4">,</span> left<span style="color:#eceff4">,</span> right<span style="color:#eceff4">)</span>
</span></span></code></pre></div><p>That&rsquo;s really it. With this addition we are able to control the amount of features used to build the decision tree.</p>
<p>The last bit now is to implement the <code>predict</code> method, which will do the actual prediction using the built tree.</p>
<h2 id="predictions" class="relative group">Predictions <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#predictions" aria-label="Anchor">#</a></span></h2>
<p>To implement the <code>predict</code> method, we will need an algorithm that will walk through the tree based on some criteria, and once it reaches a leaf node to return the value of the leaf, which wil be the class that we&rsquo;re trying to predict.</p>
<p>Let&rsquo;s see it in action:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">def</span> <span style="color:#88c0d0">predict</span><span style="color:#eceff4">(</span>node<span style="color:#eceff4">,</span> X<span style="color:#eceff4">):</span>
</span></span><span style="display:flex;"><span>    <span style="color:#81a1c1;font-weight:bold">return</span> np<span style="color:#81a1c1">.</span>array<span style="color:#eceff4">([</span>traverse_tree<span style="color:#eceff4">(</span>node<span style="color:#eceff4">,</span> x<span style="color:#eceff4">)</span> <span style="color:#81a1c1;font-weight:bold">for</span> x <span style="color:#81a1c1;font-weight:bold">in</span> X<span style="color:#eceff4">])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">def</span> <span style="color:#88c0d0">traverse_tree</span><span style="color:#eceff4">(</span>node<span style="color:#eceff4">,</span> x<span style="color:#eceff4">):</span>
</span></span><span style="display:flex;"><span>    <span style="color:#81a1c1;font-weight:bold">if</span> node<span style="color:#81a1c1">.</span>value <span style="color:#81a1c1">!=</span> <span style="color:#81a1c1;font-weight:bold">None</span><span style="color:#eceff4">:</span>
</span></span><span style="display:flex;"><span>        <span style="color:#81a1c1;font-weight:bold">return</span> node<span style="color:#81a1c1">.</span>value
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#81a1c1;font-weight:bold">if</span> x<span style="color:#eceff4">[</span>node<span style="color:#81a1c1">.</span>feature<span style="color:#eceff4">]</span> <span style="color:#81a1c1">&lt;=</span> node<span style="color:#81a1c1">.</span>threshold<span style="color:#eceff4">:</span>
</span></span><span style="display:flex;"><span>        <span style="color:#81a1c1;font-weight:bold">return</span> traverse_tree<span style="color:#eceff4">(</span>node<span style="color:#81a1c1">.</span>left<span style="color:#eceff4">,</span> x<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#81a1c1;font-weight:bold">return</span> traverse_tree<span style="color:#eceff4">(</span>node<span style="color:#81a1c1">.</span>right<span style="color:#eceff4">,</span> x<span style="color:#eceff4">)</span>
</span></span></code></pre></div><p>The algorithm here is quite simple: for every row of the input features, we will recursively traverse the tree and drill down until we run into a leaf node. Once we run into a leaf node we&rsquo;ll return just it&rsquo;s value, which is the target label. Let&rsquo;s use it on our dummy test set:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X <span style="color:#81a1c1">=</span> df<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> <span style="color:#eceff4">[</span><span style="color:#a3be8c">&#39;age&#39;</span><span style="color:#eceff4">,</span> <span style="color:#a3be8c">&#39;income&#39;</span><span style="color:#eceff4">]]</span><span style="color:#81a1c1">.</span>to_numpy<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>y <span style="color:#81a1c1">=</span> df<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> <span style="color:#a3be8c">&#39;bought&#39;</span><span style="color:#eceff4">]</span><span style="color:#81a1c1">.</span>to_numpy<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>root <span style="color:#81a1c1">=</span> build_tree<span style="color:#eceff4">(</span>X<span style="color:#eceff4">,</span> y<span style="color:#eceff4">,</span> <span style="color:#b48ead">0</span><span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>y_hat <span style="color:#81a1c1">=</span> predict<span style="color:#eceff4">(</span>root<span style="color:#eceff4">,</span> X<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accuracy <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>sum<span style="color:#eceff4">(</span>y <span style="color:#81a1c1">==</span> y_hat<span style="color:#eceff4">)</span> <span style="color:#81a1c1">/</span> <span style="color:#81a1c1">len</span><span style="color:#eceff4">(</span>y<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span><span style="color:#81a1c1">print</span><span style="color:#eceff4">(</span>accuracy<span style="color:#eceff4">)</span>
</span></span></code></pre></div><pre><code>1.0
</code></pre>
<p>100% accuracy, wow! Don&rsquo;t be fooled by it; I am committing one of the cardinal sins of machine learning &ndash; using the same data to train and test our model. We need to split our dataset into a training and validation set, build the decision tree using the training set and then measure its accuracy with the validation set.</p>
<p>Let&rsquo;s use our decision tree algorithm on a real-life dataset!</p>
<h2 id="predicting-cardiac-arrest" class="relative group">Predicting cardiac arrest <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#predicting-cardiac-arrest" aria-label="Anchor">#</a></span></h2>
<p>The <a href="https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset">Heart Attack Analysis &amp; Prediction Dataset</a> contains data relating to the patient&rsquo;s health condition and their heart attack outcome (i.e.¬†whether they did have a heart attack or not). It contains a few features:</p>
<ul>
<li><code>Age</code> - Age of the patient</li>
<li><code>Sex</code> - Sex of the patient</li>
<li><code>exang</code> - exercise induced angina (1 = yes; 0 = no)</li>
<li><code>caa</code> - number of major vessels (0-3)</li>
<li><code>cp</code> - Chest Pain type</li>
<li><code>trtbps</code> - resting blood pressure (in mm Hg)</li>
<li><code>chol</code> - cholestoral in mg/dl fetched via BMI sensor</li>
<li><code>fbs</code> - fasting blood sugar &gt; 120 mg/dl (1 = true; 0 = false)</li>
<li><code>rest_ecg</code> - resting electrocardiographic results</li>
<li><code>thalach</code> - maximum heart rate achieved</li>
</ul>
<p>The <code>target</code> column of the dataset is the dependent variable, or the heart attack outcome.</p>
<p>To download the dataset from Kaggle, we&rsquo;ll use the <code>opendataset</code> Python package:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#bf616a">!</span>pip install opendatasets <span style="color:#81a1c1">--</span>quiet
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">import</span> <span style="color:#8fbcbb">opendatasets</span> <span style="color:#81a1c1;font-weight:bold">as</span> <span style="color:#8fbcbb">od</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>od<span style="color:#81a1c1">.</span>download<span style="color:#eceff4">(</span><span style="color:#a3be8c">&#39;https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset&#39;</span><span style="color:#eceff4">)</span>
</span></span></code></pre></div><pre><code>Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds
Your Kaggle username: ilijaeftimov
Your Kaggle Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑
Downloading heart-attack-analysis-prediction-dataset.zip to ./heart-attack-analysis-prediction-dataset

100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.11k/4.11k [00:00&lt;00:00, 1.55MB/s]
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">import</span> <span style="color:#8fbcbb">pandas</span> <span style="color:#81a1c1;font-weight:bold">as</span> <span style="color:#8fbcbb">pd</span>
</span></span><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">import</span> <span style="color:#8fbcbb">numpy</span> <span style="color:#81a1c1;font-weight:bold">as</span> <span style="color:#8fbcbb">np</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>cardiac_df <span style="color:#81a1c1">=</span> pd<span style="color:#81a1c1">.</span>read_csv<span style="color:#eceff4">(</span><span style="color:#a3be8c">&#39;./heart-attack-analysis-prediction-dataset/heart.csv&#39;</span><span style="color:#eceff4">)</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>cardiac_df<span style="color:#81a1c1">.</span>head<span style="color:#eceff4">()</span>
</span></span></code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>sex</th>
      <th>cp</th>
      <th>trtbps</th>
      <th>chol</th>
      <th>fbs</th>
      <th>restecg</th>
      <th>thalachh</th>
      <th>exng</th>
      <th>oldpeak</th>
      <th>slp</th>
      <th>caa</th>
      <th>thall</th>
      <th>output</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>63</td>
      <td>1</td>
      <td>3</td>
      <td>145</td>
      <td>233</td>
      <td>1</td>
      <td>0</td>
      <td>150</td>
      <td>0</td>
      <td>2.3</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>37</td>
      <td>1</td>
      <td>2</td>
      <td>130</td>
      <td>250</td>
      <td>0</td>
      <td>1</td>
      <td>187</td>
      <td>0</td>
      <td>3.5</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>41</td>
      <td>0</td>
      <td>1</td>
      <td>130</td>
      <td>204</td>
      <td>0</td>
      <td>0</td>
      <td>172</td>
      <td>0</td>
      <td>1.4</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>56</td>
      <td>1</td>
      <td>1</td>
      <td>120</td>
      <td>236</td>
      <td>0</td>
      <td>1</td>
      <td>178</td>
      <td>0</td>
      <td>0.8</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>57</td>
      <td>0</td>
      <td>0</td>
      <td>120</td>
      <td>354</td>
      <td>0</td>
      <td>1</td>
      <td>163</td>
      <td>1</td>
      <td>0.6</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
<p>The dataset is pretty small as it contains just over 300 observations:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>cardiac_df<span style="color:#81a1c1">.</span>shape
</span></span></code></pre></div><pre><code>(303, 14)
</code></pre>
<p>What is interesting to notice is that all columns contain numerical data, which is great for classifier trees. In cases where we&rsquo;d have categorical data we&rsquo;d need to convert/encode it to numerical, e.g., by using one-hot encoding.</p>
<p>To build our classifier tree, first, we will split the columns of the dataset into labels (<code>y</code>) and features (<code>X</code>). Then, once we have those, we&rsquo;ll use sklearn&rsquo;s <code>train_test_split</code> function to split the dataset into training and validation sets.</p>
<p>We will build the tree with the <code>X_train</code> and <code>y_train</code> collections. Once created, we will run predictions using the <code>X_test</code> set, which will return the predictions. In the last step, we will measure accuracy to assess the model&rsquo;s performance.</p>
<p>Let&rsquo;s see it in action:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X <span style="color:#81a1c1">=</span> cardiac_df<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> <span style="color:#a3be8c">&#39;age&#39;</span><span style="color:#eceff4">:</span><span style="color:#a3be8c">&#39;thall&#39;</span><span style="color:#eceff4">]</span><span style="color:#81a1c1">.</span>to_numpy<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>y <span style="color:#81a1c1">=</span> cardiac_df<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> <span style="color:#a3be8c">&#39;output&#39;</span><span style="color:#eceff4">]</span><span style="color:#81a1c1">.</span>to_numpy<span style="color:#eceff4">()</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">from</span> <span style="color:#8fbcbb">sklearn.model_selection</span> <span style="color:#81a1c1;font-weight:bold">import</span> train_test_split
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X_train<span style="color:#eceff4">,</span> X_test<span style="color:#eceff4">,</span> y_train<span style="color:#eceff4">,</span> y_test <span style="color:#81a1c1">=</span> train_test_split<span style="color:#eceff4">(</span>X<span style="color:#eceff4">,</span> y<span style="color:#eceff4">,</span> test_size <span style="color:#81a1c1">=</span> <span style="color:#b48ead">0.3</span><span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>root <span style="color:#81a1c1">=</span> build_tree<span style="color:#eceff4">(</span>X<span style="color:#81a1c1">=</span>X_train<span style="color:#eceff4">,</span> y<span style="color:#81a1c1">=</span>y_train<span style="color:#eceff4">,</span> current_depth <span style="color:#81a1c1">=</span> <span style="color:#b48ead">0</span><span style="color:#eceff4">,</span> min_samples_split<span style="color:#81a1c1">=</span><span style="color:#b48ead">5</span><span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>y_hat <span style="color:#81a1c1">=</span> predict<span style="color:#eceff4">(</span>root<span style="color:#eceff4">,</span> X_test<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accuracy <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>sum<span style="color:#eceff4">(</span>y_test <span style="color:#81a1c1">==</span> y_hat<span style="color:#eceff4">)</span> <span style="color:#81a1c1">/</span> <span style="color:#81a1c1">len</span><span style="color:#eceff4">(</span>y_test<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span><span style="color:#81a1c1">print</span><span style="color:#eceff4">(</span>accuracy<span style="color:#eceff4">)</span>
</span></span></code></pre></div><pre><code>0.8021978021978022
</code></pre>
<p>Pretty cool! Our decision tree implementation has an accuracy of ~80%. In other words, it will correctly predict a heart attack condition in 80% of the patients. We can test a few different configurations of the decision tree and see which one will perform best. The levers we can pull are the depth of the tree, the minimum samples per split, and the number of features to look at.</p>
<p>We can try a few different pairs of maximum depth and minimum samples per split to test the accuracy of each tree configuration:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X <span style="color:#81a1c1">=</span> cardiac_df<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> <span style="color:#a3be8c">&#39;age&#39;</span><span style="color:#eceff4">:</span><span style="color:#a3be8c">&#39;thall&#39;</span><span style="color:#eceff4">]</span><span style="color:#81a1c1">.</span>to_numpy<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>y <span style="color:#81a1c1">=</span> cardiac_df<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> <span style="color:#a3be8c">&#39;output&#39;</span><span style="color:#eceff4">]</span><span style="color:#81a1c1">.</span>to_numpy<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X_train<span style="color:#eceff4">,</span> X_test<span style="color:#eceff4">,</span> y_train<span style="color:#eceff4">,</span> y_test <span style="color:#81a1c1">=</span> train_test_split<span style="color:#eceff4">(</span>X<span style="color:#eceff4">,</span> y<span style="color:#eceff4">,</span> test_size <span style="color:#81a1c1">=</span> <span style="color:#b48ead">0.3</span><span style="color:#eceff4">,</span> random_state<span style="color:#81a1c1">=</span><span style="color:#b48ead">31415</span><span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>best_accuracy <span style="color:#81a1c1">=</span> <span style="color:#81a1c1">-</span><span style="color:#b48ead">1</span>
</span></span><span style="display:flex;"><span>best_max_depth <span style="color:#81a1c1">=</span> <span style="color:#81a1c1">-</span><span style="color:#b48ead">1</span>
</span></span><span style="display:flex;"><span>best_min_samples_split <span style="color:#81a1c1">=</span> <span style="color:#81a1c1">-</span><span style="color:#b48ead">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">for</span> max_depth <span style="color:#81a1c1;font-weight:bold">in</span> <span style="color:#81a1c1">range</span><span style="color:#eceff4">(</span><span style="color:#b48ead">3</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">10</span><span style="color:#eceff4">):</span>
</span></span><span style="display:flex;"><span>    <span style="color:#81a1c1;font-weight:bold">for</span> min_samples_split <span style="color:#81a1c1;font-weight:bold">in</span> <span style="color:#81a1c1">range</span><span style="color:#eceff4">(</span><span style="color:#b48ead">2</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">10</span><span style="color:#eceff4">):</span>
</span></span><span style="display:flex;"><span>        root <span style="color:#81a1c1">=</span> build_tree<span style="color:#eceff4">(</span>X<span style="color:#81a1c1">=</span>X_train<span style="color:#eceff4">,</span> y<span style="color:#81a1c1">=</span>y_train<span style="color:#eceff4">,</span> current_depth <span style="color:#81a1c1">=</span> <span style="color:#b48ead">0</span><span style="color:#eceff4">,</span> max_depth <span style="color:#81a1c1">=</span> max_depth<span style="color:#eceff4">,</span> min_samples_split<span style="color:#81a1c1">=</span>min_samples_split<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>        y_hat <span style="color:#81a1c1">=</span> predict<span style="color:#eceff4">(</span>root<span style="color:#eceff4">,</span> X_test<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        accuracy <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>sum<span style="color:#eceff4">(</span>y_test <span style="color:#81a1c1">==</span> y_hat<span style="color:#eceff4">)</span> <span style="color:#81a1c1">/</span> <span style="color:#81a1c1">len</span><span style="color:#eceff4">(</span>y_test<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#81a1c1;font-weight:bold">if</span> accuracy <span style="color:#81a1c1">&gt;</span> best_accuracy<span style="color:#eceff4">:</span>
</span></span><span style="display:flex;"><span>            best_accuracy <span style="color:#81a1c1">=</span> accuracy
</span></span><span style="display:flex;"><span>            best_max_depth <span style="color:#81a1c1">=</span> max_depth
</span></span><span style="display:flex;"><span>            best_min_samples_split <span style="color:#81a1c1">=</span> min_samples_split
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span><span style="color:#81a1c1">print</span><span style="color:#eceff4">(</span><span style="color:#a3be8c">f</span><span style="color:#a3be8c">&#34;Best configration: Max depth: </span><span style="color:#a3be8c">{</span>best_max_depth<span style="color:#a3be8c">}</span><span style="color:#a3be8c"> </span><span style="color:#ebcb8b">\t</span><span style="color:#a3be8c"> Minimum samples split: </span><span style="color:#a3be8c">{</span>best_min_samples_split<span style="color:#a3be8c">}</span><span style="color:#a3be8c"> </span><span style="color:#ebcb8b">\t</span><span style="color:#a3be8c"> Accuracy: </span><span style="color:#a3be8c">{</span>best_accuracy<span style="color:#a3be8c">}</span><span style="color:#a3be8c">&#34;</span><span style="color:#eceff4">)</span>
</span></span></code></pre></div><pre><code>Best configration: Max depth: 3      Minimum samples split: 2    Accuracy: 0.8131868131868132
</code></pre>
<p>In my dummy benchmark above, it seems that the configuration of <code>max_depth</code> set at 3 and <code>min_samples_split</code> set at 2 will produce the most accurate decision tree, with an accuracy of 0.813.</p>
<p>Let&rsquo;s compare the performance of my implementation with the one from scikit-learn:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">from</span> <span style="color:#8fbcbb">sklearn</span> <span style="color:#81a1c1;font-weight:bold">import</span> tree
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X <span style="color:#81a1c1">=</span> cardiac_df<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> <span style="color:#a3be8c">&#39;age&#39;</span><span style="color:#eceff4">:</span><span style="color:#a3be8c">&#39;thall&#39;</span><span style="color:#eceff4">]</span><span style="color:#81a1c1">.</span>to_numpy<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>y <span style="color:#81a1c1">=</span> cardiac_df<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> <span style="color:#a3be8c">&#39;output&#39;</span><span style="color:#eceff4">]</span><span style="color:#81a1c1">.</span>to_numpy<span style="color:#eceff4">()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X_train<span style="color:#eceff4">,</span> X_test<span style="color:#eceff4">,</span> y_train<span style="color:#eceff4">,</span> y_test <span style="color:#81a1c1">=</span> train_test_split<span style="color:#eceff4">(</span>X<span style="color:#eceff4">,</span> y<span style="color:#eceff4">,</span> test_size <span style="color:#81a1c1">=</span> <span style="color:#b48ead">0.3</span><span style="color:#eceff4">,</span> random_state<span style="color:#81a1c1">=</span><span style="color:#b48ead">31415</span><span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>clf <span style="color:#81a1c1">=</span> tree<span style="color:#81a1c1">.</span>DecisionTreeClassifier<span style="color:#eceff4">(</span>criterion<span style="color:#81a1c1">=</span><span style="color:#a3be8c">&#39;entropy&#39;</span><span style="color:#eceff4">,</span> max_depth<span style="color:#81a1c1">=</span><span style="color:#b48ead">3</span><span style="color:#eceff4">,</span> min_samples_split<span style="color:#81a1c1">=</span><span style="color:#b48ead">2</span><span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>clf <span style="color:#81a1c1">=</span> clf<span style="color:#81a1c1">.</span>fit<span style="color:#eceff4">(</span>X_train<span style="color:#eceff4">,</span> y_train<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>pred <span style="color:#81a1c1">=</span> clf<span style="color:#81a1c1">.</span>predict<span style="color:#eceff4">(</span>X_test<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>accuracy <span style="color:#81a1c1">=</span> np<span style="color:#81a1c1">.</span>sum<span style="color:#eceff4">(</span>y_test <span style="color:#81a1c1">==</span> pred<span style="color:#eceff4">)</span> <span style="color:#81a1c1">/</span> <span style="color:#81a1c1">len</span><span style="color:#eceff4">(</span>y_test<span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>accuracy
</span></span></code></pre></div><pre><code>0.8131868131868132
</code></pre>
<p>This is pretty cool to see! The <code>DecisionTreeClassifier</code> from scikit-learn, when using the same configuration as my implementation, is as accurate as my decision tree implementation.</p>
<p>Let&rsquo;s use the same tree visualization technique we used before and see the tree that scikit-learn built for the heart attack prediction decision tree:</p>
<div class="highlight"><pre tabindex="0" style="color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">from</span> <span style="color:#8fbcbb">sklearn.tree</span> <span style="color:#81a1c1;font-weight:bold">import</span> plot_tree
</span></span><span style="display:flex;"><span><span style="color:#81a1c1;font-weight:bold">import</span> <span style="color:#8fbcbb">matplotlib.pyplot</span> <span style="color:#81a1c1;font-weight:bold">as</span> <span style="color:#8fbcbb">plt</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>features <span style="color:#81a1c1">=</span> cardiac_df<span style="color:#81a1c1">.</span>loc<span style="color:#eceff4">[:,</span> <span style="color:#a3be8c">&#39;age&#39;</span><span style="color:#eceff4">:</span><span style="color:#a3be8c">&#39;thall&#39;</span><span style="color:#eceff4">]</span><span style="color:#81a1c1">.</span>columns
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fig<span style="color:#eceff4">,</span> ax <span style="color:#81a1c1">=</span> plt<span style="color:#81a1c1">.</span>subplots<span style="color:#eceff4">(</span>figsize<span style="color:#81a1c1">=</span><span style="color:#eceff4">(</span><span style="color:#b48ead">20</span><span style="color:#eceff4">,</span> <span style="color:#b48ead">10</span><span style="color:#eceff4">),</span> dpi<span style="color:#81a1c1">=</span><span style="color:#b48ead">60</span><span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>plot_tree<span style="color:#eceff4">(</span>clf<span style="color:#eceff4">,</span> 
</span></span><span style="display:flex;"><span>          feature_names<span style="color:#81a1c1">=</span>features<span style="color:#eceff4">,</span> 
</span></span><span style="display:flex;"><span>          impurity<span style="color:#81a1c1">=</span><span style="color:#81a1c1;font-weight:bold">False</span><span style="color:#eceff4">,</span> 
</span></span><span style="display:flex;"><span>          class_names<span style="color:#81a1c1">=</span><span style="color:#eceff4">[</span><span style="color:#a3be8c">&#39;No&#39;</span><span style="color:#eceff4">,</span> <span style="color:#a3be8c">&#39;Yes&#39;</span><span style="color:#eceff4">],</span> 
</span></span><span style="display:flex;"><span>          proportion<span style="color:#81a1c1">=</span><span style="color:#81a1c1;font-weight:bold">False</span><span style="color:#eceff4">,</span> 
</span></span><span style="display:flex;"><span>          filled<span style="color:#81a1c1">=</span><span style="color:#81a1c1;font-weight:bold">True</span><span style="color:#eceff4">,</span> 
</span></span><span style="display:flex;"><span>          ax<span style="color:#81a1c1">=</span>ax<span style="color:#eceff4">,</span> 
</span></span><span style="display:flex;"><span>          label<span style="color:#81a1c1">=</span><span style="color:#a3be8c">&#39;all&#39;</span><span style="color:#eceff4">,</span> 
</span></span><span style="display:flex;"><span>          rounded<span style="color:#81a1c1">=</span><span style="color:#81a1c1;font-weight:bold">True</span><span style="color:#eceff4">)</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#81a1c1">.</span>show<span style="color:#eceff4">()</span>
</span></span></code></pre></div><p><img src="index_files/figure-markdown_strict/cell-33-output-1.png" alt=""  />
</p>
<p>Seeing how scikit-learn&rsquo;s <code>DecisionTreeClassifier</code> fitted the data to a decision tree structure is fantastic. First, it split the data using the <code>cp</code> feature (chest pain type). Then on the left branch, it continued using <code>caa</code> (number of major vessels) and <code>restecg</code> (resting electrocardiographic results) to build the tree. On the right branch it used the <code>sex</code>, <code>caa</code>, and <code>thalachh</code> (maximum heart rate achieved) features.</p>
<p>After seeing how our implementation stacks against scikit-learn&rsquo;s, we&rsquo;re in an excellent place to conclude this deep-ish dive into using classifier decision trees. Now that we understand how decision trees work inside-out, we will revisit them soon with other powerful algorithms, such as Random Forests.</p>
<h2 id="references" class="relative group">References <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#references" aria-label="Anchor">#</a></span></h2>
<p>Some useful links that I used while writing this post:</p>
<ul>
<li>[Information gain (decision tree)](<a href="https://en.wikipedia.org/wiki/Information_gain">https://en.wikipedia.org/wiki/Information_gain</a>_(decision_tree)) on Wikipedia</li>
<li><a href="https://towardsdatascience.com/implementing-a-decision-tree-from-scratch-f5358ff9c4bb">Implementing a Decision Tree From Scratch</a> on Towards Data Science</li>
<li><a href="https://github.com/rasbt/python-machine-learning-book/blob/master/faq/decision-tree-binary.md">Why are implementations of decision tree algorithms usually binary and what are the advantages of the different impurity metrics?</a> by Sebastian Raschka</li>
<li><a href="https://github.com/AssemblyAI-Examples/Machine-Learning-From-Scratch/tree/main/04%20Decision%20Trees">Decision Tree implementation</a> by Misra Turp</li>
<li><a href="https://www.youtube.com/watch?v=ZVR2Way4nwQ">Decision Tree Classification Clearly Explained!</a> by Normalized Nerd</li>
<li><a href="https://www.youtube.com/watch?v=LDRbO9a6XPU">Let&rsquo;s Write a Decision Tree Classifier from Scratch - Machine Learning Recipes #8</a> by Josh Gordon/Google</li>
<li><a href="https://www.youtube.com/watch?v=NxEHSAfFlK8">How to implement Decision Trees from scratch with Python</a> by Misra Turp/AssemblyAI</li>
</ul>

      </div>
    </section>
    <footer class="pt-8 max-w-prose print:hidden">
      
  <div class="flex">
    
      
      
        
        <img
          class="w-24 h-24 !mt-0 !mb-0 ltr:mr-4 rtl:ml-4 rounded-full"
          width="96"
          height="96"
          alt="Ilija Eftimov"
          src="/img/avatar_huae39953c6189b6aa60e47103b358d088_112612_192x192_fill_q75_box_smart1.jpg"
        />
      
    
    <div class="place-self-center">
      
        <div class="text-[0.6rem] leading-3 text-neutral-500 dark:text-neutral-400 uppercase">
          Author
        </div>
        <div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">
          Ilija Eftimov
        </div>
      
      
      <div class="text-2xl sm:text-lg">
  <div class="flex flex-wrap text-neutral-400 dark:text-neutral-500">
    
      
        <a
          class="px-1 hover:text-primary-700 dark:hover:text-primary-400"
          href="https://twitter.com/ilijaio"
          target="_blank"
          aria-label="Twitter"
          rel="me noopener noreferrer"
          >

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>

  </span>

</a
        >
      
    
      
        <a
          class="px-1 hover:text-primary-700 dark:hover:text-primary-400"
          href="https://www.linkedin.com/in/ieftimov/"
          target="_blank"
          aria-label="Linkedin"
          rel="me noopener noreferrer"
          >

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>

  </span>

</a
        >
      
    
      
        <a
          class="px-1 hover:text-primary-700 dark:hover:text-primary-400"
          href="mailto:blog@ieftimov.com"
          target="_blank"
          aria-label="Email"
          rel="me noopener noreferrer"
          >

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1c-27.64 140.9 68.65 266.2 199.1 285.1c19.01 2.888 36.17-12.26 36.17-31.49l.0001-.6631c0-15.74-11.44-28.88-26.84-31.24c-84.35-12.98-149.2-86.13-149.2-174.2c0-102.9 88.61-185.5 193.4-175.4c91.54 8.869 158.6 91.25 158.6 183.2l0 16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98 .0036c-7.299 0-13.2 4.992-15.12 11.68c-24.85-12.15-54.24-16.38-86.06-5.106c-38.75 13.73-68.12 48.91-73.72 89.64c-9.483 69.01 43.81 128 110.9 128c26.44 0 50.43-9.544 69.59-24.88c24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3C495.1 107.1 361.2-9.332 207.8 20.73zM239.1 304.3c-26.47 0-48-21.56-48-48.05s21.53-48.05 48-48.05s48 21.56 48 48.05S266.5 304.3 239.1 304.3z"/></svg>

  </span>

</a
        >
      
    
      
        <a
          class="px-1 hover:text-primary-700 dark:hover:text-primary-400"
          href="https://captainscodebook.com"
          target="_blank"
          aria-label="Substack"
          rel="me noopener noreferrer"
          >

</a
        >
      
    
      
        <a
          class="px-1 hover:text-primary-700 dark:hover:text-primary-400"
          href="https://dev.to/fteem"
          target="_blank"
          aria-label="Dev"
          rel="me noopener noreferrer"
          >

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M120.12 208.29c-3.88-2.9-7.77-4.35-11.65-4.35H91.03v104.47h17.45c3.88 0 7.77-1.45 11.65-4.35 3.88-2.9 5.82-7.25 5.82-13.06v-69.65c-.01-5.8-1.96-10.16-5.83-13.06zM404.1 32H43.9C19.7 32 .06 51.59 0 75.8v360.4C.06 460.41 19.7 480 43.9 480h360.2c24.21 0 43.84-19.59 43.9-43.8V75.8c-.06-24.21-19.7-43.8-43.9-43.8zM154.2 291.19c0 18.81-11.61 47.31-48.36 47.25h-46.4V172.98h47.38c35.44 0 47.36 28.46 47.37 47.28l.01 70.93zm100.68-88.66H201.6v38.42h32.57v29.57H201.6v38.41h53.29v29.57h-62.18c-11.16.29-20.44-8.53-20.72-19.69V193.7c-.27-11.15 8.56-20.41 19.71-20.69h63.19l-.01 29.52zm103.64 115.29c-13.2 30.75-36.85 24.63-47.44 0l-38.53-144.8h32.57l29.71 113.72 29.57-113.72h32.58l-38.46 144.8z"/></svg>

  </span>

</a
        >
      
    
      
        <a
          class="px-1 hover:text-primary-700 dark:hover:text-primary-400"
          href="https://github.com/fteem"
          target="_blank"
          aria-label="Github"
          rel="me noopener noreferrer"
          >

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>

  </span>

</a
        >
      
    
      
        <a
          class="px-1 hover:text-primary-700 dark:hover:text-primary-400"
          href="https://t.me/ieftimovcom"
          target="_blank"
          aria-label="Telegram"
          rel="me noopener noreferrer"
          >

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M248,8C111.033,8,0,119.033,0,256S111.033,504,248,504,496,392.967,496,256,384.967,8,248,8ZM362.952,176.66c-3.732,39.215-19.881,134.378-28.1,178.3-3.476,18.584-10.322,24.816-16.948,25.425-14.4,1.326-25.338-9.517-39.287-18.661-21.827-14.308-34.158-23.215-55.346-37.177-24.485-16.135-8.612-25,5.342-39.5,3.652-3.793,67.107-61.51,68.335-66.746.153-.655.3-3.1-1.154-4.384s-3.59-.849-5.135-.5q-3.283.746-104.608,69.142-14.845,10.194-26.894,9.934c-8.855-.191-25.888-5.006-38.551-9.123-15.531-5.048-27.875-7.717-26.8-16.291q.84-6.7,18.45-13.7,108.446-47.248,144.628-62.3c68.872-28.647,83.183-33.623,92.511-33.789,2.052-.034,6.639.474,9.61,2.885a10.452,10.452,0,0,1,3.53,6.716A43.765,43.765,0,0,1,362.952,176.66Z"/></svg>

  </span>

</a
        >
      
    
  </div>

</div>
    </div>
  </div>


      
  
  <section class="flex flex-row flex-wrap justify-center pt-4 text-xl">
    
      
        <a
          class="bg-neutral-300 text-neutral-700 dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800 m-1 hover:bg-primary-500 hover:text-neutral rounded min-w-[2.4rem] inline-block text-center p-1"
          href="https://www.facebook.com/sharer/sharer.php?u=https://ieftimov.com/posts/classifier-decision-trees/&amp;quote=Predict%20heart%20attack%20outcomes%20using%20decision%20tree%20classifier%20from%20scratch%20and%20scikit-learn"
          title="Share on Facebook"
          aria-label="Share on Facebook"
          >

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14 0 55.52 4.84 55.52 4.84v61h-31.28c-30.8 0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg>

  </span>

</a
        >
      
    
      
        <a
          class="bg-neutral-300 text-neutral-700 dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800 m-1 hover:bg-primary-500 hover:text-neutral rounded min-w-[2.4rem] inline-block text-center p-1"
          href="https://twitter.com/intent/tweet/?url=https://ieftimov.com/posts/classifier-decision-trees/&amp;text=Predict%20heart%20attack%20outcomes%20using%20decision%20tree%20classifier%20from%20scratch%20and%20scikit-learn"
          title="Tweet on Twitter"
          aria-label="Tweet on Twitter"
          >

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>

  </span>

</a
        >
      
    
      
        <a
          class="bg-neutral-300 text-neutral-700 dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800 m-1 hover:bg-primary-500 hover:text-neutral rounded min-w-[2.4rem] inline-block text-center p-1"
          href="https://reddit.com/submit/?url=https://ieftimov.com/posts/classifier-decision-trees/&amp;resubmit=true&amp;title=Predict%20heart%20attack%20outcomes%20using%20decision%20tree%20classifier%20from%20scratch%20and%20scikit-learn"
          title="Submit to Reddit"
          aria-label="Submit to Reddit"
          >

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M201.5 305.5c-13.8 0-24.9-11.1-24.9-24.6 0-13.8 11.1-24.9 24.9-24.9 13.6 0 24.6 11.1 24.6 24.9 0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4 0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7 0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9 0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5 0 52.6 59.2 95.2 132 95.2 73.1 0 132.3-42.6 132.3-95.2 0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6 0-2.2-2.2-6.1-2.2-8.3 0-2.5 2.5-2.5 6.4 0 8.6 22.8 22.8 87.3 22.8 110.2 0 2.5-2.2 2.5-6.1 0-8.6-2.2-2.2-6.1-2.2-8.3 0zm7.7-75c-13.6 0-24.6 11.1-24.6 24.9 0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.1 24.9-24.6 0-13.8-11-24.9-24.9-24.9z"/></svg>

  </span>

</a
        >
      
    
      
        <a
          class="bg-neutral-300 text-neutral-700 dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800 m-1 hover:bg-primary-500 hover:text-neutral rounded min-w-[2.4rem] inline-block text-center p-1"
          href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://ieftimov.com/posts/classifier-decision-trees/&amp;title=Predict%20heart%20attack%20outcomes%20using%20decision%20tree%20classifier%20from%20scratch%20and%20scikit-learn"
          title="Share on LinkedIn"
          aria-label="Share on LinkedIn"
          >

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>

  </span>

</a
        >
      
    
      
        <a
          class="bg-neutral-300 text-neutral-700 dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800 m-1 hover:bg-primary-500 hover:text-neutral rounded min-w-[2.4rem] inline-block text-center p-1"
          href="mailto:?body=https://ieftimov.com/posts/classifier-decision-trees/&amp;subject=Predict%20heart%20attack%20outcomes%20using%20decision%20tree%20classifier%20from%20scratch%20and%20scikit-learn"
          title="Send via email"
          aria-label="Send via email"
          >

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1c-27.64 140.9 68.65 266.2 199.1 285.1c19.01 2.888 36.17-12.26 36.17-31.49l.0001-.6631c0-15.74-11.44-28.88-26.84-31.24c-84.35-12.98-149.2-86.13-149.2-174.2c0-102.9 88.61-185.5 193.4-175.4c91.54 8.869 158.6 91.25 158.6 183.2l0 16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98 .0036c-7.299 0-13.2 4.992-15.12 11.68c-24.85-12.15-54.24-16.38-86.06-5.106c-38.75 13.73-68.12 48.91-73.72 89.64c-9.483 69.01 43.81 128 110.9 128c26.44 0 50.43-9.544 69.59-24.88c24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3C495.1 107.1 361.2-9.332 207.8 20.73zM239.1 304.3c-26.47 0-48-21.56-48-48.05s21.53-48.05 48-48.05s48 21.56 48 48.05S266.5 304.3 239.1 304.3z"/></svg>

  </span>

</a
        >
      
    
  </section>


      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600" />
      <div class="flex justify-between pt-3">
        <span>
          
            <a class="flex group" href="/posts/knn-from-scratch-scikit-learn/">
              <span
                class="mr-3 ltr:inline rtl:hidden text-neutral-700 dark:text-neutral group-hover:text-primary-600 dark:group-hover:text-primary-400"
                >&larr;</span
              >
              <span
                class="ml-3 ltr:hidden rtl:inline text-neutral-700 dark:text-neutral group-hover:text-primary-600 dark:group-hover:text-primary-400"
                >&rarr;</span
              >
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >Predict diabetes using k-NN from scratch and scikit-learn</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2023-03-11 00:00:00 &#43;0000 UTC">11 March 2023</time>
                  
                </span>
              </span>
            </a>
          
        </span>
        <span>
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

        
          <div
            class="absolute top-[100vh] ltr:right-0 rtl:left-0 w-12 pointer-events-none bottom-0"
          >
            <a
              href="#the-top"
              class="w-12 h-12 sticky pointer-events-auto top-[calc(100vh-5.5rem)] bg-neutral/50 dark:bg-neutral-800/50 backdrop-blur rounded-full text-xl flex items-center justify-center text-neutral-700 dark:text-neutral hover:text-primary-600 dark:hover:text-primary-400"
              aria-label="Scroll to top"
              title="Scroll to top"
            >
              &uarr;
            </a>
          </div>
        
      </main><footer class="py-10 print:hidden">
  
  
    <nav class="pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400">
      <ul class="flex flex-col list-none sm:flex-row">
        
          <li
            class="mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"
          >
            <a
              class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
              href="/analytics"
              title=""
              >Analytics</a
            >
          </li>
        
          <li
            class="mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"
          >
            <a
              class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
              href="/index.xml"
              title=""
              >RSS</a
            >
          </li>
        
          <li
            class="mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"
          >
            <a
              class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
              href="https://forms.gle/orfXK5jh1LRaiFzo8"
              title=""
              >Suggest a topic</a
            >
          </li>
        
      </ul>
    </nav>
  
  <div class="flex items-center justify-between">
    <div>
      
      
        <p class="text-sm text-neutral-500 dark:text-neutral-400">
            &copy;
            2023
            Ilija Eftimov
        </p>
      
      
      
        <p class="text-xs text-neutral-500 dark:text-neutral-400">
          
          
          Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
            href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href="https://git.io/hugo-congo" target="_blank" rel="noopener noreferrer">Congo</a>
        </p>
      
    </div>
    
    
      <div
        class="text-sm cursor-pointer text-neutral-700 dark:text-neutral hover:text-primary-600 dark:hover:text-primary-400 ltr:mr-14 rtl:ml-14"
      >
        <button
          id="appearance-switcher"
          class="w-12 h-12 "
          type="button"
          title="Switch to dark appearance"
        >
          <span class="inline dark:hidden">

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>

  </span>

</span>
          <span class="hidden dark:inline">

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>

  </span>

</span>
        </button>
      </div>
    
  </div>
  
  
</footer>
<div
  id="search-wrapper"
  class="fixed inset-0 z-50 flex flex-col p-4 sm:p-6 md:p-[10vh] lg:p-[12vh] w-screen h-screen cursor-default bg-neutral-500/50 backdrop-blur-sm dark:bg-neutral-900/50 invisible"
  data-url="https://ieftimov.com/"
>
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg border-neutral-200 top-20 bg-neutral dark:bg-neutral-800 dark:border-neutral-700"
  >
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          

  <span class="relative inline-block align-text-bottom icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-transparent focus:outline-2"
          placeholder="Search"
          tabindex="0"
        />
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 dark:text-neutral hover:text-primary-600 dark:hover:text-primary-400"
        title="Close (Esc)"
      >
        

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>

  </span>


      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
</html>
